<h1>Function Reference</h1>
<p>[checkpoint_directory](checkpoint_directory) - Set/Get Spark checkpoint directory
<p>[compile_package_jars](compile_package_jars) - Compile Scala sources into a Java Archive (jar)
<p>[connection_config](connection_config) - Read configuration values for a connection
<p>[connection_is_open](connection_is_open) - Check whether the connection is open
<p>[connection_spark_shinyapp](connection_spark_shinyapp) - A Shiny app that can be used to construct a `spark_connect` statement
<p>[copy_to.spark_connection](copy_to.spark_connection) - Copy an R Data Frame to Spark
<p>[DBISparkResult-class](DBISparkResult-class) - DBI Spark Result.
<p>[download_scalac](download_scalac) - Downloads default Scala Compilers
<p>[ensure](ensure) - Enforce Specific Structure for R Objects
<p>[find_scalac](find_scalac) - Discover the Scala Compiler
<p>[ft_binarizer](ft_binarizer) - Feature Transformation -- Binarizer
<p>[ft_bucketizer](ft_bucketizer) - Feature Transformation -- Bucketizer
<p>[ft_count_vectorizer](ft_count_vectorizer) - Feature Tranformation -- CountVectorizer
<p>[ft_discrete_cosine_transform](ft_discrete_cosine_transform) - Feature Transformation -- Discrete Cosine Transform (DCT)
<p>[ft_elementwise_product](ft_elementwise_product) - Feature Transformation -- ElementwiseProduct
<p>[ft_one_hot_encoder](ft_one_hot_encoder) - Feature Transformation -- OneHotEncoder
<p>[ft_quantile_discretizer](ft_quantile_discretizer) - Feature Transformation -- QuantileDiscretizer
<p>[ft_regex_tokenizer](ft_regex_tokenizer) - Feature Tranformation -- RegexTokenizer
<p>[ft_sql_transformer](ft_sql_transformer) - Feature Transformation -- SQLTransformer
<p>[ft_tokenizer](ft_tokenizer) - Feature Tranformation -- Tokenizer
<p>[ft_vector_assembler](ft_vector_assembler) - Feature Transformation -- VectorAssembler
<p>[hive_context_config](hive_context_config) - Runtime configuration interface for Hive
<p>[invoke_method](invoke_method) - Generic call interface for spark shell
<p>[invoke](invoke) - Invoke a Method on a JVM Object
<p>[livy_config](livy_config) - Create a Spark Configuration for Livy
<p>[livy_install](livy_install) - Install Livy
<p>[livy_service_start](livy_service) - Start Livy
<p>[ml_als_factorization](ml_als_factorization) - Spark ML -- Alternating Least Squares (ALS) matrix factorization.
<p>[ml_binary_classification_eval](ml_binary_classification_eval) - Spark ML - Binary Classification Evaluator
<p>[ml_classification_eval](ml_classification_eval) - Spark ML - Classification Evaluator
<p>[ml_create_dummy_variables](ml_create_dummy_variables) - Create Dummy Variables
<p>[ml_decision_tree](ml_decision_tree) - Spark ML -- Decision Trees
<p>[ml_generalized_linear_regression](ml_generalized_linear_regression) - Spark ML -- Generalized Linear Regression
<p>[ml_glm_tidiers](ml_glm_tidiers) - Tidying methods for Spark ML linear models
<p>[ml_gradient_boosted_trees](ml_gradient_boosted_trees) - Spark ML -- Gradient-Boosted Tree
<p>[ml_lda](ml_lda) - Spark ML -- Latent Dirichlet Allocation
<p>[ml_linear_regression](ml_linear_regression) - Spark ML -- Linear Regression
<p>[ml_logistic_regression](ml_logistic_regression) - Spark ML -- Logistic Regression
<p>[ml_model_data](ml_model_data) - Extracts data associated with a Spark ML model
<p>[ml_model](ml_model) - Create an ML Model Object
<p>[ml_multilayer_perceptron](ml_multilayer_perceptron) - Spark ML -- Multilayer Perceptron
<p>[ml_naive_bayes](ml_naive_bayes) - Spark ML -- Naive-Bayes
<p>[ml_one_vs_rest](ml_one_vs_rest) - Spark ML -- One vs Rest
<p>[ml_options](ml_options) - Options for Spark ML Routines
<p>[ml_pca](ml_pca) - Spark ML -- Principal Components Analysis
<p>[ml_prepare_dataframe](ml_prepare_dataframe) - Prepare a Spark DataFrame for Spark ML Routines
<p>[ml_prepare_response_features_intercept](ml_prepare_inputs) - Pre-process the Inputs to a Spark ML Routine
<p>[ml_random_forest](ml_random_forest) - Spark ML -- Random Forests
<p>[ml_saveload](ml_saveload) - Save / Load a Spark ML Model Fit
<p>[ml_survival_regression](ml_survival_regression) - Spark ML -- Survival Regression
<p>[ml_tree_feature_importance](ml_tree_feature_importance) - Spark ML - Feature Importance for Tree Models
<p>[na.replace](na.replace) - Replace Missing Values in Objects
<p>[%>%](pipe) - Pipe operator
<p>[print_jobj](print_jobj) - Generic method for print jobj for a connection type
<p>[reexports](reexports) - Objects exported from other packages
<p>[register_extension](register_extension) - Register a Package that Implements a Spark Extension
<p>[sdf_along](sdf_along) - Create DataFrame for along Object
<p>[sdf_bind](sdf_bind) - Bind multiple Spark DataFrames by row and column
<p>[sdf_broadcast](sdf_broadcast) - Broadcast hint
<p>[sdf_checkpoint](sdf_checkpoint) - Checkpoint a Spark DataFrame
<p>[sdf_coalesce](sdf_coalesce) - Coalesces a Spark DataFrame
<p>[sdf_copy_to](sdf_copy_to) - Copy an Object into Spark
<p>[sdf_fast_bind_cols](sdf_fast_bind_cols) - Fast cbind for Spark DataFrames
<p>[sdf_len](sdf_len) - Create DataFrame for Length
<p>[sdf_mutate](sdf_mutate) - Mutate a Spark DataFrame
<p>[sdf_num_partitions](sdf_num_partitions) - Gets number of partitions of a Spark DataFrame
<p>[sdf_partition](sdf_partition) - Partition a Spark Dataframe
<p>[sdf_persist](sdf_persist) - Persist a Spark DataFrame
<p>[sdf_pivot](sdf_pivot) - Pivot a Spark DataFrame
<p>[sdf_predict](sdf_predict) - Model Predictions with Spark DataFrames
<p>[sdf_quantile](sdf_quantile) - Compute (Approximate) Quantiles with a Spark DataFrame
<p>[sdf_read_column](sdf_read_column) - Read a Column from a Spark DataFrame
<p>[sdf_register](sdf_register) - Register a Spark DataFrame
<p>[sdf_repartition](sdf_repartition) - Repartition a Spark DataFrame
<p>[sdf_residuals.ml_model_generalized_linear_regression](sdf_residuals) - Model Residuals
<p>[sdf_sample](sdf_sample) - Randomly Sample Rows from a Spark DataFrame
<p>[sdf_schema](sdf_schema) - Read the Schema of a Spark DataFrame
<p>[sdf_separate_column](sdf_separate_column) - Separate a Vector Column into Scalar Columns
<p>[sdf_seq](sdf_seq) - Create DataFrame for Range
<p>[sdf_sort](sdf_sort) - Sort a Spark DataFrame
<p>[sdf_with_sequential_id](sdf_with_sequential_id) - Add a Sequential ID Column to a Spark DataFrame
<p>[sdf_with_unique_id](sdf_with_unique_id) - Add a Unique ID Column to a Spark DataFrame
<p>[sdf-saveload](sdf-saveload) - Save / Load a Spark DataFrame
<p>[spark_apply](spark_apply) - Apply a Function in Spark
<p>[spark_compilation_spec](spark_compilation_spec) - Define a Spark Compilation Specification
<p>[spark_compile](spark_compile) - Compile Scala sources into a Java Archive
<p>[spark_config](spark_config) - Read Spark Configuration
<p>[spark_connection](spark_connection) - Retrieve the Spark Connection Associated with an R Object
<p>[spark_context_config](spark_context_config) - Runtime configuration interface for Spark.
<p>[spark_dataframe](spark_dataframe) - Retrieve a Spark DataFrame
<p>[spark_default_compilation_spec](spark_default_compilation_spec) - Default Compilation Specification for Spark Extensions
<p>[spark_default_version](spark_default_version) - determine the version that will be used by default if version is NULL
<p>[spark_dependency](spark_dependency) - Define a Spark dependency
<p>[spark_home_dir](spark_home_dir) - Find the SPARK_HOME directory for a version of Spark
<p>[spark_home_set](spark_home_set) - Set the SPARK_HOME environment variable
<p>[spark_install](spark_install) - Download and install various versions of Spark
<p>[spark_jobj](spark_jobj) - Retrieve a Spark JVM Object Reference
<p>[spark_load_table](spark_load_table) - Reads from a Spark Table into a Spark DataFrame.
<p>[spark_log](spark_log) - View Entries in the Spark Log
<p>[spark_read_csv](spark_read_csv) - Read a CSV file into a Spark DataFrame
<p>[spark_read_jdbc](spark_read_jdbc) - Read from JDBC connection into a Spark DataFrame.
<p>[spark_read_json](spark_read_json) - Read a JSON file into a Spark DataFrame
<p>[spark_read_parquet](spark_read_parquet) - Read a Parquet file into a Spark DataFrame
<p>[spark_read_source](spark_read_source) - Read from a generic source into a Spark DataFrame.
<p>[spark_read_table](spark_read_table) - Reads from a Spark Table into a Spark DataFrame.
<p>[spark_save_table](spark_save_table) - Saves a Spark DataFrame as a Spark table
<p>[spark_version_from_home](spark_version_from_home) - Get the Spark Version Associated with a Spark Installation
<p>[spark_version](spark_version) - Get the Spark Version Associated with a Spark Connection
<p>[spark_web](spark_web) - Open the Spark web interface
<p>[spark_write_csv](spark_write_csv) - Write a Spark DataFrame to a CSV
<p>[spark_write_json](spark_write_json) - Write a Spark DataFrame to a JSON file
<p>[spark_write_parquet](spark_write_parquet) - Write a Spark DataFrame to a Parquet file
<p>[spark_write_table](spark_write_table) - Writes a Spark DataFrame into a Spark table
<p>[spark-api](spark-api) - Access the Spark API
<p>[spark-connections](spark-connections) - Manage Spark Connections
<p>[src_databases](src_databases) - Show database list
<p>[tbl_cache](tbl_cache) - Cache a Spark Table
<p>[tbl_change_db](tbl_change_db) - Use specific database
<p>[tbl_uncache](tbl_uncache) - Uncache a Spark Table
<p>[top_n](top_n) - Select top (or bottom) n rows (by value)
