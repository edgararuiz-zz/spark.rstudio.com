<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-us">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
   <link rel="stylesheet" href="../stylesheets/bootstrap.min.css">
    <title> - sparklyr: R interface for Apache Spark</title>
    <meta name="generator" content="Hugo 0.20" />

    
    <meta name="description" content="A website built through Hugo and blogdown (and gitdown).">
    
    <link rel="canonical" href="../readme/">
    

    <meta property="og:url" content="/readme/">
    <meta property="og:title" content="sparklyr: R interface for Apache Spark">
    <meta property="og:image" content="">
    <meta name="apple-mobile-web-app-title" content="sparklyr: R interface for Apache Spark">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="../images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="../images/favicon.ico">

    <link rel="stylesheet" href="../stylesheets/fonts.css">
    <link rel="stylesheet" href="../stylesheets/application.css">
    <link rel="stylesheet" href="../stylesheets/temporary.css">
    <link rel="stylesheet" href="../stylesheets/palettes.css">
    <link rel="stylesheet" href="../stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:400,700|Roboto&#43;Mono">
    <style>
      body, input {
        font-family: 'Roboto', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Roboto Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="../javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-rstudio-dark palette-accent-orange">




<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>

    <div>
      <h1>sparklyr: R interface for Apache Spark</h1>
    </div>


    
    <div class="button button-twitter" role="button" aria-label="Twitter">
       <a href="https://twitter.com/rstudio" title="@rstudio on Twitter" target="_blank" class="toggle-button icon icon-twitter"></a>
    </div>
    

    
    <div class="button button-github" role="button" aria-label="GitHub">
      <a href="https://github.com/rstudio/sparklyr" title="@rstudio/sparklyr on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
    </div>
    
    
   </div>

</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">
  <a href="../" class="project">
    <div class="banner">
      
        <div class="logo">
          <img src="sparklyr.png#ZgotmplZ">
        </div>
      
      <div class="name">
        <strong>sparklyr: R interface for Apache Spark </strong>
        
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      

      <div class="toc">
        
        <ul>
          





<li>
  
    



<a  title="Reference" href="../reference/">
	
	Reference
</a>


  
</li>



<li>
  
    



<a  title="Documents" href="../documents/">
	
	Documents
</a>


  
</li>



<li>
  
    



<a  title="News" href="../news/">
	
	News
</a>


  
</li>



<li>
  
    



<a  title="GitHub" href="https://github.com/rstudio/sparklyr">
	
	GitHub
</a>


  
</li>


        </ul>
        

        
      </div>
    </div>
  </div>
</nav>

	</div>

	<article class="article">
		<div class="wrapper">
		  <h1> </h1>
			

<h1 id="sparklyr-r-interface-for-apache-spark">sparklyr: R interface for Apache Spark</h1>

<p><a href="https://travis-ci.org/rstudio/sparklyr"><img src="https://travis-ci.org/rstudio/sparklyr.svg?branch=master" alt="Build Status" /></a> <a href="https://cran.r-project.org/package=sparklyr"><img src="https://www.r-pkg.org/badges/version/sparklyr" alt="CRAN\_Status\_Badge" /></a> <a href="https://gitter.im/rstudio/sparklyr?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge"><img src="https://badges.gitter.im/rstudio/sparklyr.svg" alt="Join the chat at https://gitter.im/rstudio/sparklyr" /></a></p>

<p><img src="tools/readme/sparklyr-illustration.png" width=364 height=197 align="right"/></p>

<ul>
<li>Connect to <a href="http://spark.apache.org/">Spark</a> from R. The sparklyr package provides a <br/> complete <a href="https://github.com/hadley/dplyr">dplyr</a> backend.</li>
<li>Filter and aggregate Spark datasets then bring them into R for <br/> analysis and visualization.</li>
<li>Use Spark&rsquo;s distributed <a href="http://spark.apache.org/docs/latest/mllib-guide.html">machine learning</a> library from R.</li>
<li>Create <a href="http://spark.rstudio.com/extensions.html">extensions</a> that call the full Spark API and provide <br/> interfaces to Spark packages.</li>
</ul>

<h2 id="installation">Installation</h2>

<p>You can install the <strong>sparklyr</strong> package from CRAN as follows:</p>

<pre><code class="language-r">install.packages(&quot;sparklyr&quot;)
</code></pre>

<p>You should also install a local version of Spark for development purposes:</p>

<pre><code class="language-r">library(sparklyr)
spark_install(version = &quot;2.1.0&quot;)
</code></pre>

<p>To upgrade to the latest version of sparklyr, run the following command and restart your r session:</p>

<pre><code class="language-r">devtools::install_github(&quot;rstudio/sparklyr&quot;)
</code></pre>

<p>If you use the RStudio IDE, you should also download the latest <a href="https://www.rstudio.com/products/rstudio/download/preview/">preview release</a> of the IDE which includes several enhancements for interacting with Spark (see the <a href="#rstudio-ide">RStudio IDE</a> section below for more details).</p>

<h2 id="connecting-to-spark">Connecting to Spark</h2>

<p>You can connect to both local instances of Spark as well as remote Spark clusters. Here we&rsquo;ll connect to a local instance of Spark via the <a href="http://spark.rstudio.com/reference/sparklyr/latest/spark_connect.html">spark_connect</a> function:</p>

<pre><code class="language-r">library(sparklyr)
sc &lt;- spark_connect(master = &quot;local&quot;)
</code></pre>

<p>The returned Spark connection (<code>sc</code>) provides a remote dplyr data source to the Spark cluster.</p>

<p>For more information on connecting to remote Spark clusters see the <a href="http://spark.rstudio.com/deployment.html">Deployment</a> section of the sparklyr website.</p>

<h2 id="using-dplyr">Using dplyr</h2>

<p>We can now use all of the available dplyr verbs against the tables within the cluster.</p>

<p>We&rsquo;ll start by copying some datasets from R into the Spark cluster (note that you may need to install the nycflights13 and Lahman packages in order to execute this code):</p>

<pre><code class="language-r">install.packages(c(&quot;nycflights13&quot;, &quot;Lahman&quot;))
</code></pre>

<pre><code class="language-r">library(dplyr)
iris_tbl &lt;- copy_to(sc, iris)
flights_tbl &lt;- copy_to(sc, nycflights13::flights, &quot;flights&quot;)
batting_tbl &lt;- copy_to(sc, Lahman::Batting, &quot;batting&quot;)
src_tbls(sc)
</code></pre>

<pre><code>## [1] &quot;batting&quot; &quot;flights&quot; &quot;iris&quot;
</code></pre>

<p>To start with here&rsquo;s a simple filtering example:</p>

<pre><code class="language-r"># filter by departure delay and print the first few records
flights_tbl %&gt;% filter(dep_delay == 2)
</code></pre>

<pre><code>## # Source:   lazy query [?? x 19]
## # Database: spark_connection
##     year month   day dep_time sched_dep_time dep_delay arr_time
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;
##  1  2013     1     1      517            515         2      830
##  2  2013     1     1      542            540         2      923
##  3  2013     1     1      702            700         2     1058
##  4  2013     1     1      715            713         2      911
##  5  2013     1     1      752            750         2     1025
##  6  2013     1     1      917            915         2     1206
##  7  2013     1     1      932            930         2     1219
##  8  2013     1     1     1028           1026         2     1350
##  9  2013     1     1     1042           1040         2     1325
## 10  2013     1     1     1231           1229         2     1523
## # ... with 6,223 more rows, and 12 more variables: sched_arr_time &lt;int&gt;,
## #   arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;,
## #   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,
## #   minute &lt;dbl&gt;, time_hour &lt;dbl&gt;
</code></pre>

<p><a href="https://CRAN.R-project.org/package=dplyr">Introduction to dplyr</a> provides additional dplyr examples you can try. For example, consider the last example from the tutorial which plots data on flight delays:</p>

<pre><code class="language-r">delay &lt;- flights_tbl %&gt;% 
  group_by(tailnum) %&gt;%
  summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %&gt;%
  filter(count &gt; 20, dist &lt; 2000, !is.na(delay)) %&gt;%
  collect

# plot delays
library(ggplot2)
ggplot(delay, aes(dist, delay)) +
  geom_point(aes(size = count), alpha = 1/2) +
  geom_smooth() +
  scale_size_area(max_size = 2)
</code></pre>

<pre><code>## `geom_smooth()` using method = 'gam'
</code></pre>

<p><img src="tools/readme/ggplot2-1.png" alt="" /></p>

<h3 id="window-functions">Window Functions</h3>

<p>dplyr <a href="https://CRAN.R-project.org/package=dplyr">window functions</a> are also supported, for example:</p>

<pre><code class="language-r">batting_tbl %&gt;%
  select(playerID, yearID, teamID, G, AB:H) %&gt;%
  arrange(playerID, yearID, teamID) %&gt;%
  group_by(playerID) %&gt;%
  filter(min_rank(desc(H)) &lt;= 2 &amp; H &gt; 0)
</code></pre>

<pre><code>## # Source:     lazy query [?? x 7]
## # Database:   spark_connection
## # Groups:     playerID
## # Ordered by: playerID, yearID, teamID
##     playerID yearID teamID     G    AB     R     H
##        &lt;chr&gt;  &lt;int&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1 abbotpa01   2000    SEA    35     5     1     2
##  2 abbotpa01   2004    PHI    10    11     1     2
##  3 abnersh01   1992    CHA    97   208    21    58
##  4 abnersh01   1990    SDN    91   184    17    45
##  5 abreujo02   2015    CHA   154   613    88   178
##  6 abreujo02   2014    CHA   145   556    80   176
##  7 acevejo01   2001    CIN    18    34     1     4
##  8 acevejo01   2004    CIN    39    43     0     2
##  9 adamsbe01   1919    PHI    78   232    14    54
## 10 adamsbe01   1918    PHI    84   227    10    40
## # ... with 2.561e+04 more rows
</code></pre>

<p>For additional documentation on using dplyr with Spark see the <a href="http://spark.rstudio.com/dplyr.html">dplyr</a> section of the sparklyr website.</p>

<h2 id="using-sql">Using SQL</h2>

<p>It&rsquo;s also possible to execute SQL queries directly against tables within a Spark cluster. The <code>spark_connection</code> object implements a <a href="https://github.com/rstats-db/DBI">DBI</a> interface for Spark, so you can use <code>dbGetQuery</code> to execute SQL and return the result as an R data frame:</p>

<pre><code class="language-r">library(DBI)
iris_preview &lt;- dbGetQuery(sc, &quot;SELECT * FROM iris LIMIT 10&quot;)
iris_preview
</code></pre>

<pre><code>##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species
## 1           5.1         3.5          1.4         0.2  setosa
## 2           4.9         3.0          1.4         0.2  setosa
## 3           4.7         3.2          1.3         0.2  setosa
## 4           4.6         3.1          1.5         0.2  setosa
## 5           5.0         3.6          1.4         0.2  setosa
## 6           5.4         3.9          1.7         0.4  setosa
## 7           4.6         3.4          1.4         0.3  setosa
## 8           5.0         3.4          1.5         0.2  setosa
## 9           4.4         2.9          1.4         0.2  setosa
## 10          4.9         3.1          1.5         0.1  setosa
</code></pre>

<h2 id="machine-learning">Machine Learning</h2>

<p>You can orchestrate machine learning algorithms in a Spark cluster via the <a href="http://spark.apache.org/docs/latest/mllib-guide.html">machine learning</a> functions within <strong>sparklyr</strong>. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning workflows.</p>

<p>Here&rsquo;s an example where we use <a href="http://spark.rstudio.com/reference/sparklyr/latest/ml_linear_regression.html">ml_linear_regression</a> to fit a linear regression model. We&rsquo;ll use the built-in <code>mtcars</code> dataset, and see if we can predict a car&rsquo;s fuel consumption (<code>mpg</code>) based on its weight (<code>wt</code>), and the number of cylinders the engine contains (<code>cyl</code>). We&rsquo;ll assume in each case that the relationship between <code>mpg</code> and each of our features is linear.</p>

<pre><code class="language-r"># copy mtcars into spark
mtcars_tbl &lt;- copy_to(sc, mtcars)

# transform our data set, and then partition into 'training', 'test'
partitions &lt;- mtcars_tbl %&gt;%
  filter(hp &gt;= 100) %&gt;%
  mutate(cyl8 = cyl == 8) %&gt;%
  sdf_partition(training = 0.5, test = 0.5, seed = 1099)

# fit a linear model to the training dataset
fit &lt;- partitions$training %&gt;%
  ml_linear_regression(response = &quot;mpg&quot;, features = c(&quot;wt&quot;, &quot;cyl&quot;))
</code></pre>

<pre><code>## * No rows dropped by 'na.omit' call
</code></pre>

<pre><code class="language-r">fit
</code></pre>

<pre><code>## Call: ml_linear_regression(., response = &quot;mpg&quot;, features = c(&quot;wt&quot;, &quot;cyl&quot;))
## 
## Coefficients:
## (Intercept)          wt         cyl 
##   37.066699   -2.309504   -1.639546
</code></pre>

<p>For linear regression models produced by Spark, we can use <code>summary()</code> to learn a bit more about the quality of our fit, and the statistical significance of each of our predictors.</p>

<pre><code class="language-r">summary(fit)
</code></pre>

<pre><code>## Call: ml_linear_regression(., response = &quot;mpg&quot;, features = c(&quot;wt&quot;, &quot;cyl&quot;))
## 
## Deviance Residuals::
##     Min      1Q  Median      3Q     Max 
## -2.6881 -1.0507 -0.4420  0.4757  3.3858 
## 
## Coefficients:
##             Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 37.06670    2.76494 13.4059 2.981e-07 ***
## wt          -2.30950    0.84748 -2.7252   0.02341 *  
## cyl         -1.63955    0.58635 -2.7962   0.02084 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-Squared: 0.8665
## Root Mean Squared Error: 1.799
</code></pre>

<p>Spark machine learning supports a wide array of algorithms and feature transformations and as illustrated above it&rsquo;s easy to chain these functions together with dplyr pipelines. To learn more see the <a href="mllib.html">machine learning</a> section.</p>

<h2 id="reading-and-writing-data">Reading and Writing Data</h2>

<p>You can read and write data in CSV, JSON, and Parquet formats. Data can be stored in HDFS, S3, or on the local filesystem of cluster nodes.</p>

<pre><code class="language-r">temp_csv &lt;- tempfile(fileext = &quot;.csv&quot;)
temp_parquet &lt;- tempfile(fileext = &quot;.parquet&quot;)
temp_json &lt;- tempfile(fileext = &quot;.json&quot;)

spark_write_csv(iris_tbl, temp_csv)
iris_csv_tbl &lt;- spark_read_csv(sc, &quot;iris_csv&quot;, temp_csv)

spark_write_parquet(iris_tbl, temp_parquet)
iris_parquet_tbl &lt;- spark_read_parquet(sc, &quot;iris_parquet&quot;, temp_parquet)

spark_write_json(iris_tbl, temp_json)
iris_json_tbl &lt;- spark_read_json(sc, &quot;iris_json&quot;, temp_json)

src_tbls(sc)
</code></pre>

<pre><code>## [1] &quot;batting&quot;      &quot;flights&quot;      &quot;iris&quot;         &quot;iris_csv&quot;    
## [5] &quot;iris_json&quot;    &quot;iris_parquet&quot; &quot;mtcars&quot;
</code></pre>

<h2 id="distributed-r">Distributed R</h2>

<p>You can execute arbitrary r code across your cluster using <code>spark_apply</code>. For example, we can apply <code>rgamma</code> over <code>iris</code> as follows:</p>

<pre><code class="language-r">spark_apply(iris_tbl, function(data) {
  data[1:4] + rgamma(1,2)
})
</code></pre>

<pre><code>## # Source:   table&lt;sparklyr_tmp_e1c5659f067d&gt; [?? x 4]
## # Database: spark_connection
##    Sepal_Length Sepal_Width Petal_Length Petal_Width
##           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;
##  1     7.708524    6.108524     4.008524    2.808524
##  2     7.508524    5.608524     4.008524    2.808524
##  3     7.308524    5.808524     3.908524    2.808524
##  4     7.208524    5.708524     4.108524    2.808524
##  5     7.608524    6.208524     4.008524    2.808524
##  6     8.008524    6.508524     4.308524    3.008524
##  7     7.208524    6.008524     4.008524    2.908524
##  8     7.608524    6.008524     4.108524    2.808524
##  9     7.008524    5.508524     4.008524    2.808524
## 10     7.508524    5.708524     4.108524    2.708524
## # ... with 140 more rows
</code></pre>

<p>We can also approximate π using <code>spark_apply</code> and <code>dplyr</code> as follows:</p>

<pre><code class="language-r">sdf_len(sc, 10000) %&gt;%
  spark_apply(function(d) rowSums(replicate(2, runif(nrow(d), min=-1, max=1)) ^ 2) &lt; 1) %&gt;%
  filter(id) %&gt;% count() %&gt;% collect() * 4 / 10000
</code></pre>

<pre><code>##        n
## 1 3.1416
</code></pre>

<h2 id="extensions">Extensions</h2>

<p>The facilities used internally by sparklyr for its dplyr and machine learning interfaces are available to extension packages. Since Spark is a general purpose cluster computing system there are many potential applications for extensions (e.g. interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).</p>

<p>Here&rsquo;s a simple example that wraps a Spark text file line counting function with an R function:</p>

<pre><code class="language-r"># write a CSV 
tempfile &lt;- tempfile(fileext = &quot;.csv&quot;)
write.csv(nycflights13::flights, tempfile, row.names = FALSE, na = &quot;&quot;)

# define an R interface to Spark line counting
count_lines &lt;- function(sc, path) {
  spark_context(sc) %&gt;% 
    invoke(&quot;textFile&quot;, path, 1L) %&gt;% 
      invoke(&quot;count&quot;)
}

# call spark to count the lines of the CSV
count_lines(sc, tempfile)
</code></pre>

<pre><code>## [1] 336777
</code></pre>

<p>To learn more about creating extensions see the <a href="http://spark.rstudio.com/extensions.html">Extensions</a> section of the sparklyr website.</p>

<h2 id="table-utilities">Table Utilities</h2>

<p>You can cache a table into memory with:</p>

<pre><code class="language-r">tbl_cache(sc, &quot;batting&quot;)
</code></pre>

<p>and unload from memory using:</p>

<pre><code class="language-r">tbl_uncache(sc, &quot;batting&quot;)
</code></pre>

<h2 id="connection-utilities">Connection Utilities</h2>

<p>You can view the Spark web console using the <code>spark_web</code> function:</p>

<pre><code class="language-r">spark_web(sc)
</code></pre>

<p>You can show the log using the <code>spark_log</code> function:</p>

<pre><code class="language-r">spark_log(sc, n = 10)
</code></pre>

<pre><code>## 17/06/08 22:48:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 87 (/var/folders/fz/v6wfsg2x1fb1rw4f6r0x4jwm0000gn/T//Rtmpgrw6EP/filee1c570759a7d.csv MapPartitionsRDD[357] at textFile at NativeMethodAccessorImpl.java:-2)
## 17/06/08 22:48:45 INFO TaskSchedulerImpl: Adding task set 87.0 with 1 tasks
## 17/06/08 22:48:45 INFO TaskSetManager: Starting task 0.0 in stage 87.0 (TID 158, localhost, partition 0,PROCESS_LOCAL, 2430 bytes)
## 17/06/08 22:48:45 INFO Executor: Running task 0.0 in stage 87.0 (TID 158)
## 17/06/08 22:48:45 INFO HadoopRDD: Input split: file:/var/folders/fz/v6wfsg2x1fb1rw4f6r0x4jwm0000gn/T/Rtmpgrw6EP/filee1c570759a7d.csv:0+33313106
## 17/06/08 22:48:45 INFO Executor: Finished task 0.0 in stage 87.0 (TID 158). 2082 bytes result sent to driver
## 17/06/08 22:48:45 INFO TaskSetManager: Finished task 0.0 in stage 87.0 (TID 158) in 107 ms on localhost (1/1)
## 17/06/08 22:48:45 INFO TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool 
## 17/06/08 22:48:45 INFO DAGScheduler: ResultStage 87 (count at NativeMethodAccessorImpl.java:-2) finished in 0.107 s
## 17/06/08 22:48:45 INFO DAGScheduler: Job 59 finished: count at NativeMethodAccessorImpl.java:-2, took 0.109289 s
</code></pre>

<p>Finally, we disconnect from Spark:</p>

<pre><code class="language-r">spark_disconnect(sc)
</code></pre>

<h2 id="rstudio-ide">RStudio IDE</h2>

<p>The latest RStudio <a href="https://www.rstudio.com/products/rstudio/download/preview/">Preview Release</a> of the RStudio IDE includes integrated support for Spark and the sparklyr package, including tools for:</p>

<ul>
<li>Creating and managing Spark connections</li>
<li>Browsing the tables and columns of Spark DataFrames</li>
<li>Previewing the first 1,000 rows of Spark DataFrames</li>
</ul>

<p>Once you&rsquo;ve installed the sparklyr package, you should find a new <strong>Spark</strong> pane within the IDE. This pane includes a <strong>New Connection</strong> dialog which can be used to make connections to local or remote Spark instances:</p>

<p><img src="tools/readme/spark-connect.png" class="screenshot" width=639 height=447/></p>

<p>Once you&rsquo;ve connected to Spark you&rsquo;ll be able to browse the tables contained within the Spark cluster:</p>

<p><img src="tools/readme/spark-tab.png" class="screenshot" width=639 height=393/></p>

<p>The Spark DataFrame preview uses the standard RStudio data viewer:</p>

<p><img src="tools/readme/spark-dataview.png" class="screenshot" width=639 height=446/></p>

<p>The RStudio IDE features for sparklyr are available now as part of the <a href="https://www.rstudio.com/products/rstudio/download/preview/">RStudio Preview Release</a>.</p>

<h2 id="using-h2o">Using H2O</h2>

<p><a href="https://cran.r-project.org/package=rsparkling">rsparkling</a> is a CRAN package from <a href="http://h2o.ai">H2O</a> that extends <a href="http://spark.rstudio.com">sparklyr</a> to provide an interface into <a href="https://github.com/h2oai/sparkling-water">Sparkling Water</a>. For instance, the following example installs, configures and runs <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html">h2o.glm</a>:</p>

<pre><code class="language-r">options(rsparkling.sparklingwater.version = &quot;2.1.0&quot;)

library(rsparkling)
library(sparklyr)
library(dplyr)
library(h2o)

sc &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.1.0&quot;)
mtcars_tbl &lt;- copy_to(sc, mtcars, &quot;mtcars&quot;)

mtcars_h2o &lt;- as_h2o_frame(sc, mtcars_tbl, strict_version_check = FALSE)

mtcars_glm &lt;- h2o.glm(x = c(&quot;wt&quot;, &quot;cyl&quot;), 
                      y = &quot;mpg&quot;,
                      training_frame = mtcars_h2o,
                      lambda_search = TRUE)
</code></pre>

<pre><code class="language-r">mtcars_glm
</code></pre>

<pre><code>## Model Details:
## ==============
## 
## H2ORegressionModel: glm
## Model ID:  GLM_model_R_1496987380367_1 
## GLM Model: summary
##     family     link                              regularization
## 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.1013 )
##                                                                lambda_search
## 1 nlambda = 100, lambda.max = 10.132, lambda.min = 0.1013, lambda.1se = -1.0
##   number_of_predictors_total number_of_active_predictors
## 1                          2                           2
##   number_of_iterations                                training_frame
## 1                    0 frame_rdd_29_803ef3b729b31412c7a28957c5054516
## 
## Coefficients: glm coefficients
##       names coefficients standardized_coefficients
## 1 Intercept    38.941654                 20.090625
## 2       cyl    -1.468783                 -2.623132
## 3        wt    -3.034558                 -2.969186
## 
## H2ORegressionMetrics: glm
## ** Reported on training data. **
## 
## MSE:  6.017684
## RMSE:  2.453097
## MAE:  1.940985
## RMSLE:  0.1114801
## Mean Residual Deviance :  6.017684
## R^2 :  0.8289895
## Null Deviance :1126.047
## Null D.o.F. :31
## Residual Deviance :192.5659
## Residual D.o.F. :29
## AIC :156.2425
</code></pre>

<pre><code class="language-r">spark_disconnect(sc)
</code></pre>

<h2 id="connecting-through-livy">Connecting through Livy</h2>

<p><a href="https://github.com/cloudera/livy">Livy</a> enables remote connections to Apache Spark clusters. Connecting to Spark clusters through Livy is <strong>under experimental development</strong> in <code>sparklyr</code>. Please post any feedback or questions as a GitHub issue as needed.</p>

<p>Before connecting to Livy, you will need the connection information to an existing service running Livy. Otherwise, to test <code>livy</code> in your local environment, you can install it and run it locally as follows:</p>

<pre><code class="language-r">livy_install()
</code></pre>

<pre><code class="language-r">livy_service_start()
</code></pre>

<p>To connect, use the Livy service address as <code>master</code> and <code>method = &quot;livy&quot;</code> in <code>spark_connect</code>. Once connection completes, use <code>sparklyr</code> as usual, for instance:</p>

<pre><code class="language-r">sc &lt;- spark_connect(master = &quot;http://localhost:8998&quot;, method = &quot;livy&quot;)
copy_to(sc, iris)
</code></pre>

<pre><code>## # Source:   table&lt;iris&gt; [?? x 5]
## # Database: spark_connection
##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species
##           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;   &lt;chr&gt;
##  1          5.1         3.5          1.4         0.2  setosa
##  2          4.9         3.0          1.4         0.2  setosa
##  3          4.7         3.2          1.3         0.2  setosa
##  4          4.6         3.1          1.5         0.2  setosa
##  5          5.0         3.6          1.4         0.2  setosa
##  6          5.4         3.9          1.7         0.4  setosa
##  7          4.6         3.4          1.4         0.3  setosa
##  8          5.0         3.4          1.5         0.2  setosa
##  9          4.4         2.9          1.4         0.2  setosa
## 10          4.9         3.1          1.5         0.1  setosa
## # ... with 140 more rows
</code></pre>

<pre><code class="language-r">spark_disconnect(sc)
</code></pre>

<p>Once you are done using <code>livy</code> locally, you should stop this service with:</p>

<pre><code class="language-r">livy_service_stop()
</code></pre>

<p>To connect to remote <code>livy</code> clusters that support basic authentication connect as:</p>

<pre><code class="language-r">config &lt;- livy_config_auth(&quot;&lt;username&gt;&quot;, &quot;&lt;password&quot;&gt;)
sc &lt;- spark_connect(master = &quot;&lt;address&gt;&quot;, method = &quot;livy&quot;, config = config)
spark_disconnect(sc)
</code></pre>

		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = '';
      var repo_id  = '';
    
    </script>

    <script src="../javascripts/application.js"></script>
    

    <script>
      
      var headers   = document.querySelectorAll('div.level2');
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            if(headers[i].id.length > 0){
              var li = document.createElement("li");
              li.setAttribute("class", "anchor");
              var a  = document.createElement("a");
              a.setAttribute("href", "#" + headers[i].id);
              a.setAttribute("title", headers[i].getElementsByTagName("h2")[0].innerHTM);
              a.innerHTML = headers[i].getElementsByTagName("h2")[0].innerHTML;
              li.appendChild(a)
              scrollspy.appendChild(li);
            }            
            
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }}
    </script>

    

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

