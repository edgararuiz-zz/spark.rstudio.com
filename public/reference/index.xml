<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sparklyr: R interface for Apache Spark</title>
    <link>/reference/</link>
    <description>Recent content on sparklyr: R interface for Apache Spark</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="/reference/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>/reference/dbisparkresult-class/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/dbisparkresult-class/</guid>
      <description>DBISparkResult-class: DBI Spark Result. Description DBI Spark Result.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/checkpoint_directory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/checkpoint_directory/</guid>
      <description> checkpoint_directory: Set/Get Spark checkpoint directory Description Set/Get Spark checkpoint directory
Usage spark_set_checkpoint_dir(sc, dir) spark_get_checkpoint_dir(sc)  Arguments    Argument Description     sc A spark_connection .   dir checkpoint directory, must be HDFS path of running on cluster    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/compile_package_jars/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/compile_package_jars/</guid>
      <description>compile_package_jars: Compile Scala sources into a Java Archive (jar) Description Compile the scala source files contained within an list() package into a Java Archive ( jar ) file that can be loaded and used within a Spark environment.
Usage compile_package_jars(..., spec = NULL)  Arguments    Argument Description     ... Optional compilation specifications, as generated by spark_compilation_spec . When no arguments are passed, spark_default_compilation_spec is used instead.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/connection_config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/connection_config/</guid>
      <description>connection_config: Read configuration values for a connection Description Read configuration values for a connection
Usage connection_config(sc, prefix, not_prefix = list())  Arguments    Argument Description     sc spark_connection   prefix Prefix to read parameters for (e.g. spark.context. , spark.sql. , etc.)   not_prefix Prefix to not include.    Value Named list of config parameters (note that if a prefix was specified then the names will not include the prefix)</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/connection_is_open/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/connection_is_open/</guid>
      <description> connection_is_open: Check whether the connection is open Description Check whether the connection is open
Usage connection_is_open(sc)  Arguments    Argument Description     sc spark_connection    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/connection_spark_shinyapp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/connection_spark_shinyapp/</guid>
      <description> connection_spark_shinyapp: A Shiny app that can be used to construct a spark_connect statement Description A Shiny app that can be used to construct a spark_connect statement
Usage connection_spark_shinyapp()  </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/copy_to.spark_connection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/copy_to.spark_connection/</guid>
      <description>copy_to.spark_connection: Copy an R Data Frame to Spark Description Copy an R data.frame to Spark, and return a reference to the generated Spark DataFrame as a tbl_spark . The returned object will act as a dplyr -compatible interface to the underlying Spark table.
Usage list(list(&amp;quot;copy_to&amp;quot;), list(&amp;quot;spark_connection&amp;quot;))(dest, df, name = deparse(substitute(df)), overwrite = FALSE, memory = TRUE, repartition = 0L, ...)  Arguments    Argument Description     dest A spark_connection .</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/download_scalac/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/download_scalac/</guid>
      <description>download_scalac: Downloads default Scala Compilers Description compile_package_jars requires several versions of the scala compiler to work, this is to match Spark scala versions. To help setup your environment, this function will download the required compilers under the default search path.
Usage download_scalac()  Details See find_scalac for a list of paths searched and used by this function to install the required compilers.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ensure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ensure/</guid>
      <description>ensure: Enforce Specific Structure for R Objects Description These routines are useful when preparing to pass objects to a Spark routine, as it is often necessary to ensure certain parameters are scalar integers, or scalar doubles, and so on.
Usage ensure_scalar_integer(object, allow.na = FALSE, allow.null = FALSE, default = NULL) ensure_scalar_double(object, allow.na = FALSE, allow.null = FALSE, default = NULL) ensure_scalar_boolean(object, allow.na = FALSE, allow.null = FALSE, default = NULL) ensure_scalar_character(object, allow.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/find_scalac/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/find_scalac/</guid>
      <description>find_scalac: Discover the Scala Compiler Description Find the scalac compiler for a particular version of scala , by scanning some common directories containing scala installations.
Usage find_scalac(version, locations = NULL)  Arguments    Argument Description     version The scala version to search for. Versions of the form major.minor will be matched against the scalac installation with version major.minor.patch ; if multiple compilers are discovered the most recent one will be used.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ft_binarizer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ft_binarizer/</guid>
      <description>ft_binarizer: Feature Transformation &amp;ndash; Binarizer Description Apply thresholding to a column, such that values less than or equal to the threshold are assigned the value 0.0, and values greater than the threshold are assigned the value 1.0.
Usage ft_binarizer(x, input.col = NULL, output.col = NULL, threshold = 0.5, ...)  Arguments    Argument Description     x An object (usually a spark_tbl ) coercable to a Spark DataFrame.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ft_bucketizer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ft_bucketizer/</guid>
      <description>ft_bucketizer: Feature Transformation &amp;ndash; Bucketizer Description Similar to list() &amp;rsquo;s cut function, this transforms a numeric column into a discretized column, with breaks specified through the splits parameter.
Usage ft_bucketizer(x, input.col = NULL, output.col = NULL, splits, ...)  Arguments    Argument Description     x An object (usually a spark_tbl ) coercable to a Spark DataFrame.   input.col The name of the input column(s).</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ft_count_vectorizer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ft_count_vectorizer/</guid>
      <description>ft_count_vectorizer: Feature Tranformation &amp;ndash; CountVectorizer Description Extracts a vocabulary from document collections.
Usage ft_count_vectorizer(x, input.col = NULL, output.col = NULL, min.df = NULL, min.tf = NULL, vocab.size = NULL, ...)  Arguments    Argument Description     x An object (usually a spark_tbl ) coercable to a Spark DataFrame.   input.col The name of the input column(s).   output.col The name of the output column.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ft_discrete_cosine_transform/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ft_discrete_cosine_transform/</guid>
      <description>ft_discrete_cosine_transform: Feature Transformation &amp;ndash; Discrete Cosine Transform (DCT) Description Transform a column in the time domain into another column in the frequency domain.
Usage ft_discrete_cosine_transform(x, input.col = NULL, output.col = NULL, inverse = FALSE, ...)  Arguments    Argument Description     x An object (usually a spark_tbl ) coercable to a Spark DataFrame.   input.col The name of the input column(s).   output.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ft_elementwise_product/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ft_elementwise_product/</guid>
      <description>ft_elementwise_product: Feature Transformation &amp;ndash; ElementwiseProduct Description Computes the element-wise product between two columns. Generally, this is intended as a scaling transformation, where an input vector is scaled by another vector, but this should apply for all element-wise product transformations.
Usage ft_elementwise_product(x, input.col = NULL, output.col = NULL, scaling.col, ...)  Arguments    Argument Description     x An object (usually a spark_tbl ) coercable to a Spark DataFrame.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ft_index_to_string/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ft_index_to_string/</guid>
      <description>ft_index_to_string: Feature Transformation &amp;ndash; IndexToString Description Symmetrically to ft_string_indexer , ft_index_to_string maps a column of label indices back to a column containing the original labels as strings.
Usage ft_index_to_string(x, input.col = NULL, output.col = NULL, ...)  Arguments    Argument Description     x An object (usually a spark_tbl ) coercable to a Spark DataFrame.   input.col The name of the input column(s).   output.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ft_one_hot_encoder/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ft_one_hot_encoder/</guid>
      <description>ft_one_hot_encoder: Feature Transformation &amp;ndash; OneHotEncoder Description One-hot encoding maps a column of label indices to a column of binary vectors, with at most a single one-value. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features.
Usage ft_one_hot_encoder(x, input.col = NULL, output.col = NULL, drop.last = TRUE, ...)  Arguments    Argument Description     x An object (usually a spark_tbl ) coercable to a Spark DataFrame.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ft_quantile_discretizer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ft_quantile_discretizer/</guid>
      <description>ft_quantile_discretizer: Feature Transformation &amp;ndash; QuantileDiscretizer Description Takes a column with continuous features and outputs a column with binned categorical features. The bin ranges are chosen by taking a sample of the data and dividing it into roughly equal parts. The lower and upper bin bounds will be -Infinity and +Infinity, covering all real values. This attempts to find numBuckets partitions based on a sample of the given input data, but it may find fewer depending on the data sample values.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ft_regex_tokenizer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ft_regex_tokenizer/</guid>
      <description>ft_regex_tokenizer: Feature Tranformation &amp;ndash; RegexTokenizer Description A regex based tokenizer that extracts tokens either by using the provided regex pattern to split the text (default) or repeatedly matching the regex (if gaps is false). Optional parameters also allow filtering tokens using a minimal length. It returns an array of strings that can be empty.
Usage ft_regex_tokenizer(x, input.col = NULL, output.col = NULL, pattern, ...)  Arguments    Argument Description     x An object (usually a spark_tbl ) coercable to a Spark DataFrame.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ft_sql_transformer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ft_sql_transformer/</guid>
      <description>ft_sql_transformer: Feature Transformation &amp;ndash; SQLTransformer Description Transform a data set using SQL. Use the __THIS__ placeholder as a proxy for the active table.
Usage ft_sql_transformer(x, input.col = NULL, output.col = NULL, sql, ...)  Arguments    Argument Description     x An object (usually a spark_tbl ) coercable to a Spark DataFrame.   input.col The name of the input column(s).   output.col The name of the output column.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ft_string_indexer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ft_string_indexer/</guid>
      <description>ft_string_indexer: Feature Transformation &amp;ndash; StringIndexer Description Encode a column of labels into a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, with the most frequent label assigned index 0. The transformation can be reversed with ft_index_to_string .
Usage ft_string_indexer(x, input.col = NULL, output.col = NULL, params = NULL, ...)  Arguments    Argument Description     x An object (usually a spark_tbl ) coercable to a Spark DataFrame.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ft_tokenizer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ft_tokenizer/</guid>
      <description>ft_tokenizer: Feature Tranformation &amp;ndash; Tokenizer Description A tokenizer that converts the input string to lowercase and then splits it by white spaces.
Usage ft_tokenizer(x, input.col = NULL, output.col = NULL, ...)  Arguments    Argument Description     x An object (usually a spark_tbl ) coercable to a Spark DataFrame.   input.col The name of the input column(s).   output.col The name of the output column.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ft_vector_assembler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ft_vector_assembler/</guid>
      <description>ft_vector_assembler: Feature Transformation &amp;ndash; VectorAssembler Description Combine multiple vectors into a single row-vector; that is, where each row element of the newly generated column is a vector formed by concatenating each row element from the specified input columns.
Usage ft_vector_assembler(x, input.col = NULL, output.col = NULL, ...)  Arguments    Argument Description     x An object (usually a spark_tbl ) coercable to a Spark DataFrame.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/hive_context_config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/hive_context_config/</guid>
      <description> hive_context_config: Runtime configuration interface for Hive Description Retrieves the runtime configuration interface for Hive.
Usage hive_context_config(sc)  Arguments    Argument Description     sc A spark_connection .    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/invoke/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/invoke/</guid>
      <description>invoke: Invoke a Method on a JVM Object Description Invoke methods on Java object references. These functions provide a mechanism for invoking various Java object methods directly from list() .
Usage invoke(jobj, method, ...) invoke_static(sc, class, method, ...) invoke_new(sc, class, ...)  Arguments    Argument Description     jobj An list() object acting as a Java object reference (typically, a spark_jobj ).   method The name of the method to be invoked.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/invoke_method/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/invoke_method/</guid>
      <description>invoke_method: Generic call interface for spark shell Description Generic call interface for spark shell
Usage invoke_method(sc, static, object, method, ...)  Arguments    Argument Description     sc spark_connection   static Is this a static method call (including a constructor). If so then the object parameter should be the name of a class (otherwise it should be a spark_jobj instance).   object Object instance or name of class (for static )   method Name of method   .</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/livy_config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/livy_config/</guid>
      <description>livy_config: Create a Spark Configuration for Livy Description Create a Spark Configuration for Livy
Usage livy_config(config = spark_config(), username = NULL, password = NULL)  Arguments    Argument Description     config Optional base configuration   username The username to use in the Authorization header   password The password to use in the Authorization header    Details Extends a Spark &amp;quot;spark_config&amp;quot; configuration with settings for Livy.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/livy_install/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/livy_install/</guid>
      <description>livy_install: Install Livy Description Automatically download and install list(&amp;ldquo;livy&amp;rdquo;) . livy provides a REST API to Spark.
Find the LIVY_HOME directory for a given version of Livy that was previously installed using livy_install .
Usage livy_install(version = &amp;quot;0.3.0&amp;quot;, spark_home = NULL, spark_version = NULL) livy_available_versions() livy_install_dir() livy_installed_versions() livy_home_dir(version = NULL)  Arguments    Argument Description     version The version of livy to be installed.   spark_home The path to a Spark installation.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/livy_service/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/livy_service/</guid>
      <description> livy_service_start: Start Livy Description Starts the livy service.
Stops the running instances of the livy service.
Usage livy_service_start(version = NULL, spark_version = NULL) livy_service_stop()  Arguments    Argument Description     version The version of livy to use.   spark_version The version of spark to connect to.    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_als_factorization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_als_factorization/</guid>
      <description>ml_als_factorization: Spark ML &amp;ndash; Alternating Least Squares (ALS) matrix factorization. Description Perform alternating least squares matrix factorization on a Spark DataFrame.
Usage ml_als_factorization(x, rating.column = &amp;quot;rating&amp;quot;, user.column = &amp;quot;user&amp;quot;, item.column = &amp;quot;item&amp;quot;, rank = 10L, regularization.parameter = 0.1, implicit.preferences = FALSE, alpha = 1, nonnegative = FALSE, iter.max = 10L, ml.options = ml_options(), ...)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_binary_classification_eval/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_binary_classification_eval/</guid>
      <description>ml_binary_classification_eval: Spark ML - Binary Classification Evaluator Description See the Spark ML Documentation BinaryClassificationEvaluator
Usage ml_binary_classification_eval(predicted_tbl_spark, label, score, metric = &amp;quot;areaUnderROC&amp;quot;)  Arguments    Argument Description     predicted_tbl_spark The result of running sdf_predict   label Name of column string specifying which column contains the true, indexed labels (ie 0 / 1)   score Name of column contains the scored probability of a success (ie 1)   metric The classification metric - one of: areaUnderRoc (default) or areaUnderPR    Value area under the specified curve</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_classification_eval/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_classification_eval/</guid>
      <description>ml_classification_eval: Spark ML - Classification Evaluator Description See the Spark ML Documentation MulticlassClassificationEvaluator
Usage ml_classification_eval(predicted_tbl_spark, label, predicted_lbl, metric = &amp;quot;f1&amp;quot;)  Arguments    Argument Description     predicted_tbl_spark A tbl_spark object that contains a columns with predicted labels   label Name of the column that contains the true, indexed label. Support for binary and multi-class labels, column should be of double type (use as.double)   predicted_lbl Name of the column that contains the predicted label NOT the scored probability.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_create_dummy_variables/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_create_dummy_variables/</guid>
      <description>ml_create_dummy_variables: Create Dummy Variables Description Given a column in a Spark DataFrame, generate a new Spark DataFrame containing dummy variable columns.
Usage ml_create_dummy_variables(x, input, reference = NULL, levels = NULL, labels = NULL, envir = new.env(parent = emptyenv()))  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   input The name of the input column.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_decision_tree/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_decision_tree/</guid>
      <description>ml_decision_tree: Spark ML &amp;ndash; Decision Trees Description Perform regression or classification using decision trees.
Usage ml_decision_tree(x, response, features, max.bins = 32L, max.depth = 5L, type = c(&amp;quot;auto&amp;quot;, &amp;quot;regression&amp;quot;, &amp;quot;classification&amp;quot;), ml.options = ml_options(), ...)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   response The name of the response vector (as a length-one character vector), or a formula, giving a symbolic description of the model to be fitted.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_generalized_linear_regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_generalized_linear_regression/</guid>
      <description>ml_generalized_linear_regression: Spark ML &amp;ndash; Generalized Linear Regression Description Perform generalized linear regression on a Spark DataFrame.
Usage ml_generalized_linear_regression(x, response, features, intercept = TRUE, family = gaussian(link = &amp;quot;identity&amp;quot;), weights.column = NULL, iter.max = 100L, ml.options = ml_options(), ...)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   response The name of the response vector (as a length-one character vector), or a formula, giving a symbolic description of the model to be fitted.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_glm_tidiers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_glm_tidiers/</guid>
      <description>ml_glm_tidiers: Tidying methods for Spark ML linear models Description These methods summarize the results of Spark ML models into tidy forms.
Usage list(list(&amp;quot;tidy&amp;quot;), list(&amp;quot;ml_model_generalized_linear_regression&amp;quot;))(x, exponentiate = FALSE, ...) list(list(&amp;quot;tidy&amp;quot;), list(&amp;quot;ml_model_linear_regression&amp;quot;))(x, ...) list(list(&amp;quot;augment&amp;quot;), list(&amp;quot;ml_model_generalized_linear_regression&amp;quot;))(x, newdata = NULL, type.residuals = c(&amp;quot;working&amp;quot;, &amp;quot;deviance&amp;quot;, &amp;quot;pearson&amp;quot;, &amp;quot;response&amp;quot;), ...) list(list(&amp;quot;augment&amp;quot;), list(&amp;quot;ml_model_linear_regression&amp;quot;))(x, newdata = NULL, type.residuals = c(&amp;quot;working&amp;quot;, &amp;quot;deviance&amp;quot;, &amp;quot;pearson&amp;quot;, &amp;quot;response&amp;quot;), ...) list(list(&amp;quot;glance&amp;quot;), list(&amp;quot;ml_model_generalized_linear_regression&amp;quot;))(x, ...) list(list(&amp;quot;glance&amp;quot;), list(&amp;quot;ml_model_linear_regression&amp;quot;))(x, ...)  Arguments    Argument Description     x a Spark ML model.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_gradient_boosted_trees/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_gradient_boosted_trees/</guid>
      <description>ml_gradient_boosted_trees: Spark ML &amp;ndash; Gradient-Boosted Tree Description Perform regression or classification using gradient-boosted trees.
Usage ml_gradient_boosted_trees(x, response, features, max.bins = 32L, max.depth = 5L, type = c(&amp;quot;auto&amp;quot;, &amp;quot;regression&amp;quot;, &amp;quot;classification&amp;quot;), ml.options = ml_options(), ...)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   response The name of the response vector (as a length-one character vector), or a formula, giving a symbolic description of the model to be fitted.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_lda/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_lda/</guid>
      <description>ml_lda: Spark ML &amp;ndash; Latent Dirichlet Allocation Description Fit a Latent Dirichlet Allocation (LDA) model to a Spark DataFrame.
Usage ml_lda(x, features = tbl_vars(x), k = length(features), alpha = (50/k) + 1, beta = 0.1 + 1, ml.options = ml_options(), ...)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   features The name of features (terms) to use for the model fit.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_linear_regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_linear_regression/</guid>
      <description>ml_linear_regression: Spark ML &amp;ndash; Linear Regression Description Perform linear regression on a Spark DataFrame.
Usage ml_linear_regression(x, response, features, intercept = TRUE, alpha = 0, lambda = 0, weights.column = NULL, iter.max = 100L, ml.options = ml_options(), ...)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   response The name of the response vector (as a length-one character vector), or a formula, giving a symbolic description of the model to be fitted.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_logistic_regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_logistic_regression/</guid>
      <description>ml_logistic_regression: Spark ML &amp;ndash; Logistic Regression Description Perform logistic regression on a Spark DataFrame.
Usage ml_logistic_regression(x, response, features, intercept = TRUE, alpha = 0, lambda = 0, weights.column = NULL, iter.max = 100L, ml.options = ml_options(), ...)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   response The name of the response vector (as a length-one character vector), or a formula, giving a symbolic description of the model to be fitted.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_model/</guid>
      <description>ml_model: Create an ML Model Object Description Create an ML model object, wrapping the result of a Spark ML routine call. The generated object will be an list() list with S3 classes c(&amp;quot;ml_model_&amp;lt;class&amp;gt;&amp;quot;, &amp;quot;ml_model&amp;quot;) .
Usage ml_model(class, model, ..., .call = sys.call(sys.parent()))  Arguments    Argument Description     class The name of the machine learning routine used in the encompassing model. Note that the model name generated will be generated as ml_model_&amp;lt;class&amp;gt; ; that is, ml_model will be prefixed.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_model_data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_model_data/</guid>
      <description>ml_model_data: Extracts data associated with a Spark ML model Description Extracts data associated with a Spark ML model
Usage ml_model_data(object)  Arguments    Argument Description     object a Spark ML model    Value A tbl_spark</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_multilayer_perceptron/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_multilayer_perceptron/</guid>
      <description>ml_multilayer_perceptron: Spark ML &amp;ndash; Multilayer Perceptron Description Creates and trains multilayer perceptron on a Spark DataFrame.
Usage ml_multilayer_perceptron(x, response, features, layers, iter.max = 100, seed = sample(.Machine$integer.max, 1), ml.options = ml_options(), ...)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   response The name of the response vector (as a length-one character vector), or a formula, giving a symbolic description of the model to be fitted.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_naive_bayes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_naive_bayes/</guid>
      <description>ml_naive_bayes: Spark ML &amp;ndash; Naive-Bayes Description Perform regression or classification using naive bayes.
Usage ml_naive_bayes(x, response, features, lambda = 0, ml.options = ml_options(), ...)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   response The name of the response vector (as a length-one character vector), or a formula, giving a symbolic description of the model to be fitted.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_one_vs_rest/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_one_vs_rest/</guid>
      <description>ml_one_vs_rest: Spark ML &amp;ndash; One vs Rest Description Perform regression or classification using one vs rest.
Usage ml_one_vs_rest(x, classifier, response, features, ml.options = ml_options(), ...)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   classifier The classifier model. These model objects can be obtained through the use of the only.model parameter supplied with ml_options .</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_options/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_options/</guid>
      <description>ml_options: Options for Spark ML Routines Description Provide this object to the various Spark ML methods, to control certain facets of the model outputs produced.
Usage ml_options(id.column = random_string(&amp;quot;id&amp;quot;), response.column = random_string(&amp;quot;response&amp;quot;), features.column = random_string(&amp;quot;features&amp;quot;), model.transform = NULL, only.model = FALSE, na.action = getOption(&amp;quot;na.action&amp;quot;, &amp;quot;na.omit&amp;quot;), ...)  Arguments    Argument Description     id.column The name to assign to the generated id column.   response.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_pca/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_pca/</guid>
      <description>ml_pca: Spark ML &amp;ndash; Principal Components Analysis Description Perform principal components analysis on a Spark DataFrame.
Usage ml_pca(x, features = tbl_vars(x), ml.options = ml_options(), ...)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   features The columns to use in the principal components analysis. Defaults to all columns in x .   ml.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_prepare_dataframe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_prepare_dataframe/</guid>
      <description>ml_prepare_dataframe: Prepare a Spark DataFrame for Spark ML Routines Description This routine prepares a Spark DataFrame for use by Spark ML routines.
Usage ml_prepare_dataframe(x, features, response = NULL, ..., ml.options = ml_options(), envir = new.env(parent = emptyenv()))  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   features The name of features (terms) to use for the model fit.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_prepare_inputs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_prepare_inputs/</guid>
      <description>ml_prepare_response_features_intercept: Pre-process the Inputs to a Spark ML Routine Description Pre-process / normalize the inputs typically passed to a Spark ML routine.
Usage ml_prepare_response_features_intercept(x = NULL, response, features, intercept, envir = parent.frame(), categorical.transformations = new.env(parent = emptyenv()), ml.options = ml_options()) ml_prepare_features(x, features, envir = parent.frame(), ml.options = ml_options())  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_random_forest/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_random_forest/</guid>
      <description>ml_random_forest: Spark ML &amp;ndash; Random Forests Description Perform regression or classification using random forests with a Spark DataFrame.
Usage ml_random_forest(x, response, features, max.bins = 32L, max.depth = 5L, num.trees = 20L, type = c(&amp;quot;auto&amp;quot;, &amp;quot;regression&amp;quot;, &amp;quot;classification&amp;quot;), ml.options = ml_options(), ...)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   response The name of the response vector (as a length-one character vector), or a formula, giving a symbolic description of the model to be fitted.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_saveload/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_saveload/</guid>
      <description>ml_saveload: Save / Load a Spark ML Model Fit Description Save / load a ml_model fit.
Usage ml_load(sc, file, meta = ml_load_meta(file)) ml_save(model, file, meta = ml_save_meta(model, file))  Arguments    Argument Description     sc A spark_connection .   file The path where the Spark model should be serialized / deserialized.   meta The path where the list() metadata should be serialized / deserialized.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_survival_regression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_survival_regression/</guid>
      <description>ml_survival_regression: Spark ML &amp;ndash; Survival Regression Description Perform survival regression on a Spark DataFrame, using an Accelerated failure time (AFT) model with potentially right-censored data.
Usage ml_survival_regression(x, response, features, intercept = TRUE, censor = &amp;quot;censor&amp;quot;, iter.max = 100L, ml.options = ml_options(), ...)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   response The name of the response vector (as a length-one character vector), or a formula, giving a symbolic description of the model to be fitted.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/ml_tree_feature_importance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/ml_tree_feature_importance/</guid>
      <description>ml_tree_feature_importance: Spark ML - Feature Importance for Tree Models Description Spark ML - Feature Importance for Tree Models
Usage ml_tree_feature_importance(sc, model)  Arguments    Argument Description     sc A spark_connection .   model An ml_model encapsulating the output from a decision tree.    Value A sorted data frame with feature labels and their relative importance.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/na.replace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/na.replace/</guid>
      <description> na.replace: Replace Missing Values in Objects Description This S3 generic provides an interface for replacing NA values within an object.
Usage na.replace(object, ...)  Arguments    Argument Description     object An list() object.   ... Arguments passed along to implementing methods.    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/pipe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/pipe/</guid>
      <description> %&amp;gt;%: Pipe operator Description See %&amp;gt;% for more details.
Usage lhs %&amp;gt;% rhs  </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/print_jobj/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/print_jobj/</guid>
      <description> print_jobj: Generic method for print jobj for a connection type Description Generic method for print jobj for a connection type
Usage print_jobj(sc, jobj, ...)  Arguments    Argument Description     sc spark_connection (used for type dispatch)   jobj Object to print    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/reexports/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/reexports/</guid>
      <description>reexports: Objects exported from other packages Description These objects are imported from other packages. Follow the links below to see their documentation.
list(&amp;rdquo;\n&amp;rdquo;, &amp;ldquo; &amp;ldquo;, list(list(&amp;ldquo;broom&amp;rdquo;), list(list(list(&amp;ldquo;tidy&amp;rdquo;)), &amp;ldquo;, &amp;ldquo;, list(list(&amp;ldquo;augment&amp;rdquo;)), &amp;ldquo;, &amp;ldquo;, list(list(&amp;ldquo;glance&amp;rdquo;)))), &amp;ldquo;\n&amp;rdquo;)</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/register_extension/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/register_extension/</guid>
      <description>register_extension: Register a Package that Implements a Spark Extension Description Registering an extension package will result in the package being automatically scanned for spark dependencies when a connection to Spark is created.
Usage register_extension(package) registered_extensions()  Arguments    Argument Description     package The package(s) to register.    Note Packages should typically register their extensions in their .onLoad hook &amp;ndash; this ensures that their extensions are registered when their namespaces are loaded.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf-saveload/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf-saveload/</guid>
      <description>sdf-saveload: Save / Load a Spark DataFrame Description Routines for saving and loading Spark DataFrames.
Usage sdf_save_table(x, name, overwrite = FALSE, append = FALSE) sdf_load_table(sc, name) sdf_save_parquet(x, path, overwrite = FALSE, append = FALSE) sdf_load_parquet(sc, path)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   name The table name to assign to the saved Spark DataFrame.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_along/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_along/</guid>
      <description> sdf_along: Create DataFrame for along Object Description Creates a DataFrame along the given object.
Usage sdf_along(sc, along, repartition = NULL)  Arguments    Argument Description     sc The associated Spark connection.   along Takes the length from the length of this argument.   repartition The number of partitions to use when distributing the data across the Spark cluster.    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_bind/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_bind/</guid>
      <description>sdf_bind: Bind multiple Spark DataFrames by row and column Description sdf_bind_rows() and sdf_bind_cols() are implementation of the common pattern of do.call(rbind, sdfs) or do.call(cbind, sdfs) for binding many Spark DataFrames into one.
Usage sdf_bind_rows(..., id = NULL) sdf_bind_cols(...)  Arguments    Argument Description     ... Spark tbls to combine. Each argument can either be a Spark DataFrame or a list of Spark DataFrames When row-binding, columns are matched by name, and any missing columns with be filled with NA.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_broadcast/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_broadcast/</guid>
      <description> sdf_broadcast: Broadcast hint Description Used to force broadcast hash joins.
Usage sdf_broadcast(x)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_checkpoint/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_checkpoint/</guid>
      <description> sdf_checkpoint: Checkpoint a Spark DataFrame Description Checkpoint a Spark DataFrame
Usage sdf_checkpoint(x, eager = TRUE)  Arguments    Argument Description     x an object coercible to a Spark DataFrame   eager whether to truncate the lineage of the DataFrame    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_coalesce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_coalesce/</guid>
      <description> sdf_coalesce: Coalesces a Spark DataFrame Description Coalesces a Spark DataFrame
Usage sdf_coalesce(x, partitions)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   partitions number of partitions    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_copy_to/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_copy_to/</guid>
      <description>sdf_copy_to: Copy an Object into Spark Description Copy an object into Spark, and return an list() object wrapping the copied object (typically, a Spark DataFrame).
Usage sdf_copy_to(sc, x, name, memory, repartition, overwrite, ...) sdf_import(x, sc, name, memory, repartition, overwrite, ...)  Arguments    Argument Description     sc The associated Spark connection.   x An list() object from which a Spark DataFrame can be generated.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_fast_bind_cols/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_fast_bind_cols/</guid>
      <description> sdf_fast_bind_cols: Fast cbind for Spark DataFrames Description This is a version of sdf_bind_cols that works by zipping RDDs. From the API docs: &amp;ldquo;Assumes that the two RDDs have the same number of partitions and the same number of elements in each partition (e.g. one was made through a map on the other).&amp;rdquo;
Usage sdf_fast_bind_cols(...)  Arguments    Argument Description     ... Spark DataFrames to cbind    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_last_index/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_last_index/</guid>
      <description>sdf_last_index: Returns the last index of a Spark DataFrame Description Returns the last index of a Spark DataFrame. The Spark mapPartitionsWithIndex function is used to iterate through the last nonempty partition of the RDD to find the last record.
Usage sdf_last_index(x, id = &amp;quot;id&amp;quot;)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   id The name of the index column.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_len/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_len/</guid>
      <description> sdf_len: Create DataFrame for Length Description Creates a DataFrame for the given length.
Usage sdf_len(sc, length, repartition = NULL)  Arguments    Argument Description     sc The associated Spark connection.   length The desired length of the sequence.   repartition The number of partitions to use when distributing the data across the Spark cluster.    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_mutate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_mutate/</guid>
      <description>sdf_mutate: Mutate a Spark DataFrame Description Use Spark&amp;rsquo;s feature transformers to mutate a Spark DataFrame.
Usage sdf_mutate(.data, ...) sdf_mutate_(.data, ..., .dots)  Arguments    Argument Description     .data A spark_tbl .   ... Named arguments, mapping new column names to the transformation to be applied.   .dots A named list, mapping output names to transformations.    Seealso Other feature transformation routines: ft_binarizer , ft_bucketizer , ft_count_vectorizer , ft_discrete_cosine_transform , ft_elementwise_product , ft_index_to_string , ft_one_hot_encoder , ft_quantile_discretizer , ft_regex_tokenizer , ft_sql_transformer , ft_string_indexer , ft_tokenizer , ft_vector_assembler</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_num_partitions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_num_partitions/</guid>
      <description> sdf_num_partitions: Gets number of partitions of a Spark DataFrame Description Gets number of partitions of a Spark DataFrame
Usage sdf_num_partitions(x)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_partition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_partition/</guid>
      <description>sdf_partition: Partition a Spark Dataframe Description Partition a Spark DataFrame into multiple groups. This routine is useful for splitting a DataFrame into, for example, training and test datasets.
Usage sdf_partition(x, ..., weights = NULL, seed = sample(.Machine$integer.max, 1))  Arguments    Argument Description     x An object coercable to a Spark DataFrame.   ... Named parameters, mapping table names to weights. The weights will be normalized such that they sum to 1.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_persist/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_persist/</guid>
      <description>sdf_persist: Persist a Spark DataFrame Description Persist a Spark DataFrame, forcing any pending computations and (optionally) serializing the results to disk.
Usage sdf_persist(x, storage.level = &amp;quot;MEMORY_AND_DISK&amp;quot;)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   storage.level The storage level to be used. Please view the Spark Documentation for information on what storage levels are accepted.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_pivot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_pivot/</guid>
      <description>sdf_pivot: Pivot a Spark DataFrame Description Construct a pivot table over a Spark Dataframe, using a syntax similar to that from reshape2::dcast .
Usage sdf_pivot(x, formula, fun.aggregate = &amp;quot;count&amp;quot;)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   formula A two-sided list() formula of the form x_1 + x_2 + ... ~ y_1 .</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_predict/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_predict/</guid>
      <description>sdf_predict: Model Predictions with Spark DataFrames Description Given a ml_model fit alongside a new data set, produce a new Spark DataFrame with predicted values encoded in the &amp;quot;prediction&amp;quot; column.
Usage sdf_predict(object, newdata, ...)  Arguments    Argument Description     object, newdata An object coercable to a Spark DataFrame.   ... Optional arguments; currently unused.    Seealso Other Spark data frames: sdf_copy_to , sdf_partition , sdf_register , sdf_sample , sdf_sort</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_quantile/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_quantile/</guid>
      <description>sdf_quantile: Compute (Approximate) Quantiles with a Spark DataFrame Description Given a numeric column within a Spark DataFrame, compute approximate quantiles (to some relative error).
Usage sdf_quantile(x, column, probabilities = c(0, 0.25, 0.5, 0.75, 1), relative.error = 1e-05)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   column The column for which quantiles should be computed.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_read_column/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_read_column/</guid>
      <description> sdf_read_column: Read a Column from a Spark DataFrame Description Read a single column from a Spark DataFrame, and return the contents of that column back to list() .
Usage sdf_read_column(x, column)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   column The name of a column within x .    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_register/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_register/</guid>
      <description>sdf_register: Register a Spark DataFrame Description Registers a Spark DataFrame (giving it a table name for the Spark SQL context), and returns a tbl_spark .
Usage sdf_register(x, name = NULL)  Arguments    Argument Description     x A Spark DataFrame.   name A name to assign this table.    Seealso Other Spark data frames: sdf_copy_to , sdf_partition , sdf_predict , sdf_sample , sdf_sort</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_repartition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_repartition/</guid>
      <description> sdf_repartition: Repartition a Spark DataFrame Description Repartition a Spark DataFrame
Usage sdf_repartition(x, partitions = NULL, partition_by = NULL)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   partitions number of partitions   partition_by vector of column names used for partitioning, only supported for Spark 2.0+    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_residuals/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_residuals/</guid>
      <description> sdf_residuals.ml_model_generalized_linear_regression: Model Residuals Description This generic method returns a Spark DataFrame with model residuals added as a column to the model training data.
Usage list(list(&amp;quot;sdf_residuals&amp;quot;), list(&amp;quot;ml_model_generalized_linear_regression&amp;quot;))(object, type = c(&amp;quot;deviance&amp;quot;, &amp;quot;pearson&amp;quot;, &amp;quot;working&amp;quot;, &amp;quot;response&amp;quot;), ...) list(list(&amp;quot;sdf_residuals&amp;quot;), list(&amp;quot;ml_model_linear_regression&amp;quot;))(object, ...) sdf_residuals(object, ...)  Arguments    Argument Description     object Spark ML model object.   type type of residuals which should be returned.   ... additional arguments    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_sample/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_sample/</guid>
      <description>sdf_sample: Randomly Sample Rows from a Spark DataFrame Description Draw a random sample of rows (with or without replacement) from a Spark DataFrame.
Usage sdf_sample(x, fraction = 1, replacement = TRUE, seed = NULL)  Arguments    Argument Description     x An object coercable to a Spark DataFrame.   fraction The fraction to sample.   replacement Boolean; sample with replacement?   seed An (optional) integer seed.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_schema/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_schema/</guid>
      <description>sdf_schema: Read the Schema of a Spark DataFrame Description Read the schema of a Spark DataFrame.
Usage sdf_schema(x)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).    Details The type column returned gives the string representation of the underlying Spark type for that column; for example, a vector of numeric values would be returned with the type &amp;quot;DoubleType&amp;quot; .</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_separate_column/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_separate_column/</guid>
      <description>sdf_separate_column: Separate a Vector Column into Scalar Columns Description Given a vector column in a Spark DataFrame, split that into n separate columns, each column made up of the different elements in the column column .
Usage sdf_separate_column(x, column, into = NULL)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).   column The name of a (vector-typed) column.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_seq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_seq/</guid>
      <description>sdf_seq: Create DataFrame for Range Description Creates a DataFrame for the given range
Usage sdf_seq(sc, from = 1L, to = 1L, by = 1L, repartition = NULL)  Arguments    Argument Description     sc The associated Spark connection.   from, to The start and end to use as a range   by The increment of the sequence.   repartition The number of partitions to use when distributing the data across the Spark cluster.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_sort/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_sort/</guid>
      <description>sdf_sort: Sort a Spark DataFrame Description Sort a Spark DataFrame by one or more columns, with each column sorted in ascending order.
Usage sdf_sort(x, columns)  Arguments    Argument Description     x An object coercable to a Spark DataFrame.   columns The column(s) to sort by.    Seealso Other Spark data frames: sdf_copy_to , sdf_partition , sdf_predict , sdf_register , sdf_sample</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_with_sequential_id/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_with_sequential_id/</guid>
      <description>sdf_with_sequential_id: Add a Sequential ID Column to a Spark DataFrame Description Add a sequential ID column to a Spark DataFrame. The Spark zipWithIndex function is used to produce these. This differs from sdf_with_unique_id in that the IDs generated are independent of partitioning.
Usage sdf_with_sequential_id(x, id = &amp;quot;id&amp;quot;, from = 1L)  Arguments    Argument Description     x An object coercable to a Spark DataFrame (typically, a tbl_spark ).</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/sdf_with_unique_id/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/sdf_with_unique_id/</guid>
      <description>sdf_with_unique_id: Add a Unique ID Column to a Spark DataFrame Description Add a unique ID column to a Spark DataFrame. The Spark monotonicallyIncreasingId function is used to produce these and is guaranteed to produce unique, monotonically increasing ids; however, there is no guarantee that these IDs will be sequential. The table is persisted immediately after the column is generated, to ensure that the column is stable &amp;ndash; otherwise, it can differ across new computations.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark-api/</guid>
      <description>spark-api: Access the Spark API Description Access the commonly-used Spark objects associated with a Spark instance. These objects provide access to different facets of the Spark API.
Usage spark_context(sc) java_context(sc) hive_context(sc) spark_session(sc)  Arguments    Argument Description     sc A spark_connection .    Details The Scala API documentation is useful for discovering what methods are available for each of these objects. Use invoke to call methods on these objects.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark-connections/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark-connections/</guid>
      <description>spark-connections: Manage Spark Connections Description These routines allow you to manage your connections to Spark.
Usage spark_connect(master = &amp;quot;local&amp;quot;, spark_home = Sys.getenv(&amp;quot;SPARK_HOME&amp;quot;), method = c(&amp;quot;shell&amp;quot;, &amp;quot;livy&amp;quot;, &amp;quot;databricks&amp;quot;, &amp;quot;test&amp;quot;), app_name = &amp;quot;sparklyr&amp;quot;, version = NULL, hadoop_version = NULL, config = spark_config(), extensions = sparklyr::registered_extensions(), ...) spark_connection_is_open(sc) spark_disconnect(sc, ...) spark_disconnect_all()  Arguments    Argument Description     master Spark cluster url to connect to. Use &amp;quot;local&amp;quot; to connect to a local instance of Spark installed via spark_install .</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_apply/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_apply/</guid>
      <description>spark_apply: Apply a Function in Spark Description Applies a function to a Spark object (typically, a Spark DataFrame).
Usage spark_apply(x, f, names = colnames(x), memory = TRUE, ...)  Arguments    Argument Description     x An object (usually a spark_tbl ) coercable to a Spark DataFrame.   f A function that transforms a data frame partition into a data frame.   names The column names for the transformed object, defaults to the names from the original object.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_compilation_spec/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_compilation_spec/</guid>
      <description>spark_compilation_spec: Define a Spark Compilation Specification Description For use with compile_package_jars . The Spark compilation specification is used when compiling Spark extension Java Archives, and defines which versions of Spark, as well as which versions of Scala, should be used for compilation.
Usage spark_compilation_spec(spark_version = NULL, spark_home = NULL, scalac_path = NULL, scala_filter = NULL, jar_name = NULL, jar_path = NULL, jar_dep = NULL)  Arguments    Argument Description     spark_version The Spark version to build against.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_compile/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_compile/</guid>
      <description>spark_compile: Compile Scala sources into a Java Archive Description Given a set of scala source files, compile them into a Java Archive ( jar ).
Usage spark_compile(jar_name, spark_home = NULL, filter = NULL, scalac = NULL, jar = NULL, jar_dep = NULL)  Arguments    Argument Description     spark_home The path to the Spark sources to be used alongside compilation.   filter An optional function, used to filter out discovered scala files during compilation.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_config/</guid>
      <description>spark_config: Read Spark Configuration Description Read Spark Configuration
Usage spark_config(file = &amp;quot;config.yml&amp;quot;, use_default = TRUE)  Arguments    Argument Description     file Name of the configuration file   use_default TRUE to use the built-in detaults provided in this package    Details Read Spark configuration using the list(&amp;ldquo;config&amp;rdquo;) package.
Value Named list with configuration data</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_connection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_connection/</guid>
      <description> spark_connection: Retrieve the Spark Connection Associated with an R Object Description Retrieve the spark_connection associated with an list() object.
Usage spark_connection(x, ...)  Arguments    Argument Description     x An list() object from which a spark_connection can be obtained.   ... Optional arguments; currently unused.    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_context_config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_context_config/</guid>
      <description> spark_context_config: Runtime configuration interface for Spark. Description Retrieves the runtime configuration interface for Spark.
Usage spark_context_config(sc)  Arguments    Argument Description     sc A spark_connection .    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_dataframe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_dataframe/</guid>
      <description>spark_dataframe: Retrieve a Spark DataFrame Description This S3 generic is used to access a Spark DataFrame object (as a Java object reference) from an list() object.
Usage spark_dataframe(x, ...)  Arguments    Argument Description     x An list() object wrapping, or containing, a Spark DataFrame.   ... Optional arguments; currently unused.    Value A spark_jobj representing a Java object reference to a Spark DataFrame.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_default_compilation_spec/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_default_compilation_spec/</guid>
      <description> spark_default_compilation_spec: Default Compilation Specification for Spark Extensions Description This is the default compilation specification used for Spark extensions, when used with compile_package_jars .
Usage spark_default_compilation_spec(pkg = infer_active_package_name())  Arguments    Argument Description     pkg The package containing Spark extensions to be compiled.    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_default_version/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_default_version/</guid>
      <description> spark_default_version: determine the version that will be used by default if version is NULL Description determine the version that will be used by default if version is NULL
Usage spark_default_version()  </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_dependency/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_dependency/</guid>
      <description>spark_dependency: Define a Spark dependency Description Define a Spark dependency consisting of a set of custom JARs and Spark packages.
Usage spark_dependency(jars = NULL, packages = NULL)  Arguments    Argument Description     jars Character vector of full paths to JAR files   packages Character vector of Spark packages names    Value An object of type spark_dependency</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_home_dir/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_home_dir/</guid>
      <description>spark_home_dir: Find the SPARK_HOME directory for a version of Spark Description Find the SPARK_HOME directory for a given version of Spark that was previously installed using spark_install .
Usage spark_home_dir(version = NULL, hadoop_version = NULL)  Arguments    Argument Description     version Version of Spark   hadoop_version Version of Hadoop    Value Path to SPARK_HOME (or NULL if the specified version was not found).</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_home_set/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_home_set/</guid>
      <description>spark_home_set: Set the SPARK_HOME environment variable Description Set the SPARK_HOME environment variable. This slightly speeds up some operations, including the connection time.
Usage spark_home_set(path = NULL, verbose = getOption(&amp;quot;sparklyr.verbose&amp;quot;, is.null(path)))  Arguments    Argument Description     path A string containing the path to the installation location of Spark. If NULL , the path to the most latest Spark/Hadoop versions is used.   verbose Logical.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_install/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_install/</guid>
      <description>spark_install: Download and install various versions of Spark Description Install versions of Spark for use with local Spark connections (i.e. spark_connect(master = &amp;quot;local&amp;quot; )
Usage spark_install(version = NULL, hadoop_version = NULL, reset = TRUE, logging = &amp;quot;INFO&amp;quot;, verbose = interactive()) spark_uninstall(version, hadoop_version) spark_install_dir() spark_install_tar(tarfile) spark_installed_versions() spark_available_versions()  Arguments    Argument Description     version Version of Spark to install. See spark_available_versions for a list of supported versions   hadoop_version Version of Hadoop to install.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_jobj/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_jobj/</guid>
      <description>spark_jobj: Retrieve a Spark JVM Object Reference Description This S3 generic is used for accessing the underlying Java Virtual Machine (JVM) Spark objects associated with list() objects. These objects act as references to Spark objects living in the JVM. Methods on these objects can be called with the invoke family of functions.
Usage spark_jobj(x, ...)  Arguments    Argument Description     x An list() object containing, or wrapping, a spark_jobj .</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_load_table/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_load_table/</guid>
      <description>spark_load_table: Reads from a Spark Table into a Spark DataFrame. Description Reads from a Spark Table into a Spark DataFrame.
Usage spark_load_table(sc, name, path, options = list(), repartition = 0, memory = TRUE, overwrite = TRUE)  Arguments    Argument Description     sc A spark_connection .   name The name to assign to the newly generated table.   path The path to the file.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_log/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_log/</guid>
      <description>spark_log: View Entries in the Spark Log Description View the most recent entries in the Spark log. This can be useful when inspecting output / errors produced by Spark during the invocation of various commands.
Usage spark_log(sc, n = 100, filter = NULL, ...)  Arguments    Argument Description     sc A spark_connection .   n The max number of log entries to retrieve.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_read_csv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_read_csv/</guid>
      <description>spark_read_csv: Read a CSV file into a Spark DataFrame Description Read a tabular data file into a Spark DataFrame.
Usage spark_read_csv(sc, name, path, header = TRUE, columns = NULL, infer_schema = TRUE, delimiter = &amp;quot;,&amp;quot;, quote = &amp;quot;\&amp;quot;&amp;quot;, escape = &amp;quot;\\&amp;quot;, charset = &amp;quot;UTF-8&amp;quot;, null_value = NULL, options = list(), repartition = 0, memory = TRUE, overwrite = TRUE)  Arguments    Argument Description     sc A spark_connection .</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_read_jdbc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_read_jdbc/</guid>
      <description>spark_read_jdbc: Read from JDBC connection into a Spark DataFrame. Description Read from JDBC connection into a Spark DataFrame.
Usage spark_read_jdbc(sc, name, options = list(), repartition = 0, memory = TRUE, overwrite = TRUE)  Arguments    Argument Description     sc A spark_connection .   name The name to assign to the newly generated table.   options A list of strings with additional options.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_read_json/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_read_json/</guid>
      <description>spark_read_json: Read a JSON file into a Spark DataFrame Description Read a table serialized in the c(&amp;rdquo;JavaScript\n&amp;rdquo;, &amp;ldquo;Object Notation&amp;rdquo;) format into a Spark DataFrame.
Usage spark_read_json(sc, name, path, options = list(), repartition = 0, memory = TRUE, overwrite = TRUE)  Arguments    Argument Description     sc A spark_connection .   name The name to assign to the newly generated table.   path The path to the file.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_read_parquet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_read_parquet/</guid>
      <description>spark_read_parquet: Read a Parquet file into a Spark DataFrame Description Read a Parquet file into a Spark DataFrame.
Usage spark_read_parquet(sc, name, path, options = list(), repartition = 0, memory = TRUE, overwrite = TRUE)  Arguments    Argument Description     sc A spark_connection .   name The name to assign to the newly generated table.   path The path to the file. Needs to be accessible from the cluster.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_read_source/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_read_source/</guid>
      <description>spark_read_source: Read from a generic source into a Spark DataFrame. Description Read from a generic source into a Spark DataFrame.
Usage spark_read_source(sc, name, source, options = list(), repartition = 0, memory = TRUE, overwrite = TRUE)  Arguments    Argument Description     sc A spark_connection .   name The name to assign to the newly generated table.   source A data source capable of reading data.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_read_table/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_read_table/</guid>
      <description>spark_read_table: Reads from a Spark Table into a Spark DataFrame. Description Reads from a Spark Table into a Spark DataFrame.
Usage spark_read_table(sc, name, options = list(), repartition = 0, memory = TRUE, overwrite = TRUE)  Arguments    Argument Description     sc A spark_connection .   name The name to assign to the newly generated table.   options A list of strings with additional options.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_save_table/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_save_table/</guid>
      <description>spark_save_table: Saves a Spark DataFrame as a Spark table Description Saves a Spark DataFrame and as a Spark table.
Usage spark_save_table(x, path, mode = NULL, options = list())  Arguments    Argument Description     x A Spark DataFrame or dplyr operation   path The path to the file. Needs to be accessible from the cluster. Supports the &amp;ldquo;hdfs://&amp;rdquo; , &amp;ldquo;s3n://&amp;rdquo; and &amp;ldquo;file://&amp;rdquo; protocols.   mode Specifies the behavior when data or table already exists.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_version/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_version/</guid>
      <description>spark_version: Get the Spark Version Associated with a Spark Connection Description Retrieve the version of Spark associated with a Spark connection.
Usage spark_version(sc)  Arguments    Argument Description     sc A spark_connection .    Details Suffixes for e.g. preview versions, or snapshotted versions, are trimmed &amp;ndash; if you require the full Spark version, you can retrieve it with invoke(spark_context(sc), &amp;quot;version&amp;quot;) .
Value The Spark version as a numeric_version .</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_version_from_home/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_version_from_home/</guid>
      <description> spark_version_from_home: Get the Spark Version Associated with a Spark Installation Description Retrieve the version of Spark associated with a Spark installation.
Usage spark_version_from_home(spark_home, default = NULL)  Arguments    Argument Description     spark_home The path to a Spark installation.   default The default version to be inferred, in case version lookup failed, e.g. no Spark installation was found at spark_home .    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_web/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_web/</guid>
      <description> spark_web: Open the Spark web interface Description Open the Spark web interface
Usage spark_web(sc, ...)  Arguments    Argument Description     sc A spark_connection .   ... Optional arguments; currently unused.    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_write_csv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_write_csv/</guid>
      <description>spark_write_csv: Write a Spark DataFrame to a CSV Description Write a Spark DataFrame to a tabular (typically, comma-separated) file.
Usage spark_write_csv(x, path, header = TRUE, delimiter = &amp;quot;,&amp;quot;, quote = &amp;quot;\&amp;quot;&amp;quot;, escape = &amp;quot;\\&amp;quot;, charset = &amp;quot;UTF-8&amp;quot;, null_value = NULL, options = list(), mode = NULL, partition_by = NULL, ...)  Arguments    Argument Description     x A Spark DataFrame or dplyr operation   path The path to the file.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_write_json/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_write_json/</guid>
      <description>spark_write_json: Write a Spark DataFrame to a JSON file Description Serialize a Spark DataFrame to the c(&amp;rdquo;JavaScript\n&amp;rdquo;, &amp;ldquo;Object Notation&amp;rdquo;) format.
Usage spark_write_json(x, path, mode = NULL, options = list(), partition_by = NULL, ...)  Arguments    Argument Description     x A Spark DataFrame or dplyr operation   path The path to the file. Needs to be accessible from the cluster. Supports the &amp;ldquo;hdfs://&amp;rdquo; , &amp;ldquo;s3n://&amp;rdquo; and &amp;ldquo;file://&amp;rdquo; protocols.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_write_parquet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_write_parquet/</guid>
      <description>spark_write_parquet: Write a Spark DataFrame to a Parquet file Description Serialize a Spark DataFrame to the Parquet format.
Usage spark_write_parquet(x, path, mode = NULL, options = list(), partition_by = NULL, ...)  Arguments    Argument Description     x A Spark DataFrame or dplyr operation   path The path to the file. Needs to be accessible from the cluster. Supports the &amp;ldquo;hdfs://&amp;rdquo; , &amp;ldquo;s3n://&amp;rdquo; and &amp;ldquo;file://&amp;rdquo; protocols.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/spark_write_table/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/spark_write_table/</guid>
      <description>spark_write_table: Writes a Spark DataFrame into a Spark table Description Writes a Spark DataFrame into a Spark table.
Usage spark_write_table(x, name, mode = NULL, options = list(), partition_by = NULL, ...)  Arguments    Argument Description     x A Spark DataFrame or dplyr operation   name The name to assign to the newly generated table.   mode Specifies the behavior when data or table already exists.</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/src_databases/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/src_databases/</guid>
      <description> src_databases: Show database list Description Show database list
Usage src_databases(sc, ...)  Arguments    Argument Description     sc A spark_connection .   ... Optional arguments; currently unused.    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/tbl_cache/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/tbl_cache/</guid>
      <description>tbl_cache: Cache a Spark Table Description Force a Spark table with name name to be loaded into memory. Operations on cached tables should normally (although not always) be more performant than the same operation performed on an uncached table.
Usage tbl_cache(sc, name, force = TRUE)  Arguments    Argument Description     sc A spark_connection .   name The table name.   force Force the data to be loaded into memory?</description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/tbl_change_db/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/tbl_change_db/</guid>
      <description> tbl_change_db: Use specific database Description Use specific database
Usage tbl_change_db(sc, name)  Arguments    Argument Description     sc A spark_connection .   name The database name.    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/tbl_uncache/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/tbl_uncache/</guid>
      <description> tbl_uncache: Uncache a Spark Table Description Force a Spark table with name name to be unloaded from memory.
Usage tbl_uncache(sc, name)  Arguments    Argument Description     sc A spark_connection .   name The table name.    </description>
    </item>
    
    <item>
      <title></title>
      <link>/reference/top_n/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/reference/top_n/</guid>
      <description>top_n: Select top (or bottom) n rows (by value) Description This is a convenient wrapper that uses [filter()] and [min_rank()] to select the top or bottom entries in each group, ordered by wt.
Usage top_n(x, n, wt)  Arguments    Argument Description     x a [tbl()] to filter   n number of rows to return. If x is grouped, this is the number of rows per group.</description>
    </item>
    
  </channel>
</rss>