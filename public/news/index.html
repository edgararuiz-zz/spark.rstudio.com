<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-us">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
   <link rel="stylesheet" href="../stylesheets/bootstrap.min.css">
    <title> - sparklyr: R interface for Apache Spark</title>
    <meta name="generator" content="Hugo 0.20" />

    
    <meta name="description" content="A website built through Hugo and blogdown (and gitdown).">
    
    <link rel="canonical" href="../news/">
    

    <meta property="og:url" content="/news/">
    <meta property="og:title" content="sparklyr: R interface for Apache Spark">
    <meta property="og:image" content="">
    <meta name="apple-mobile-web-app-title" content="sparklyr: R interface for Apache Spark">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="../images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="../images/favicon.ico">

    <link rel="stylesheet" href="../stylesheets/fonts.css">
    <link rel="stylesheet" href="../stylesheets/application.css">
    <link rel="stylesheet" href="../stylesheets/temporary.css">
    <link rel="stylesheet" href="../stylesheets/palettes.css">
    <link rel="stylesheet" href="../stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:400,700|Roboto&#43;Mono">
    <style>
      body, input {
        font-family: 'Roboto', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Roboto Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="../javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-rstudio-dark palette-accent-orange">




<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>

    <div>
      <h1><a href="../">sparklyr: R interface for Apache Spark</a></h1>
    </div>


    
    <div class="button button-twitter" role="button" aria-label="Twitter">
       <a href="https://twitter.com/rstudio" title="@rstudio on Twitter" target="_blank" class="toggle-button icon icon-twitter"></a>
    </div>
    

    
    <div class="button button-github" role="button" aria-label="GitHub">
      <a href="https://github.com/rstudio/sparklyr" title="@rstudio/sparklyr on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
    </div>
    
    
   </div>

</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">

  <div class="scrollable">
    <div class="wrapper">
      

      <div class="toc">
        
        <ul>
          





<li>
  
    



<a  title="Reference" href="../reference/">
	
	Reference
</a>


  
</li>



<li>
  
    



<a  title="Documents" href="../documents/">
	
	Documents
</a>


  
</li>



<li>
  
    



<a class="current" title="News" href="../news/">
	
	News
</a>

<ul id="scrollspy">
</ul>


  
</li>


        </ul>
        

        
      </div>
    </div>
  </div>
</nav>

	</div>

	<article class="article">
		<div class="wrapper">
		  <h1> </h1>
			

<h1 id="sparklyr-0-6-0-unreleased">Sparklyr 0.6.0 (UNRELEASED)</h1>

<h3 id="distributed-r">Distributed R</h3>

<ul>
<li>Added <code>spark_apply()</code>, allowing users to use R code to directly
manipulate and transform Spark DataFrames.</li>
</ul>

<h3 id="external-data">External Data</h3>

<ul>
<li><p>Added <code>partition_by</code> parameter to <code>spark_write_csv()</code>, <code>spark_write_json()</code>,
<code>spark_write_table()</code> and <code>spark_write_parquet()</code>.</p></li>

<li><p>Added <code>spark_read_source()</code>. This function reads data from a
Spark data source which can be loaded through an Spark package.</p></li>

<li><p>Added support for <code>mode = &quot;overwrite&quot;</code> and <code>mode = &quot;append&quot;</code> to
<code>spark_write_csv()</code>.</p></li>

<li><p><code>spark_write_table()</code> now supports saving to default Hive path.</p></li>

<li><p>Improved performance of <code>spark_read_csv()</code> reading remote data when
<code>infer_schema = FALSE</code>.</p></li>

<li><p>Added <code>spark_read_jdbc()</code>. This function reads from a JDBC connection
into a Spark DataFrame.</p></li>

<li><p>Renamed <code>spark_load_table()</code> and <code>spark_save_table()</code> into <code>spark_read_table()</code>
and <code>spark_write_table()</code> for consistency with existing <code>spark_read_*()</code> and
<code>spark_write_*()</code> functions.</p></li>

<li><p>Added support to specify a vector of column names in <code>spark_read_csv()</code> to
specify column names without having to set the type of each column.</p></li>

<li><p>Improved <code>copy_to()</code>, <code>sdf_copy_to()</code> and <code>dbWriteTable()</code> performance under
<code>yarn-client</code> mode.</p></li>
</ul>

<h3 id="dplyr">dplyr</h3>

<ul>
<li><p>Support for <code>cumprod()</code> to calculate cumulative products.</p></li>

<li><p>Support for <code>cor()</code>, <code>cov()</code>, <code>sd()</code> and <code>var()</code> as window functions.</p></li>

<li><p>Support for Hive built-in operators <code>%like%</code>, <code>%rlike%</code>, and
<code>%regexp%</code> for matching regular expressions in <code>filter()</code> and <code>mutate()</code>.</p></li>

<li><p>Support for dplyr (&gt;= 0.6) which among many improvements, increases
performance in some queries by making use of a new query optimizer.</p></li>

<li><p><code>sample_frac()</code> takes a fraction instead of a percent to match dplyr.</p></li>

<li><p>Improved performance of <code>sample_n()</code> and <code>sample_frac()</code> through the use of
<code>TABLESAMPLE</code> in the generated query.</p></li>
</ul>

<h3 id="databases">Databases</h3>

<ul>
<li><p>Added <code>src_databases()</code>. This function list all the available databases.</p></li>

<li><p>Added <code>tbl_change_db()</code>. This function changes current database.</p></li>
</ul>

<h3 id="dataframes">DataFrames</h3>

<ul>
<li><p>Added <code>sdf_len()</code>, <code>sdf_seq()</code> and <code>sdf_along()</code> to help generate numeric
sequences as Spark DataFrames.</p></li>

<li><p>Added <code>spark_set_checkpoint_dir()</code>, <code>spark_get_checkpoint_dir()</code>, and
<code>sdf_checkpoint()</code> to enable checkpointing.</p></li>

<li><p>Added <code>sdf_broadcast()</code> which can be used to hint the query
optimizer to perform a broadcast join in cases where a shuffle
hash join is planned but not optimal.</p></li>

<li><p>Added <code>sdf_repartition()</code>, <code>sdf_coalesce()</code>, and <code>sdf_num_partitions()</code>
to support repartitioning and getting the number of partitions of Spark
DataFrames.</p></li>

<li><p>Added <code>sdf_bind_rows()</code> and <code>sdf_bind_cols()</code> &ndash; these functions
are the <code>sparklyr</code> equivalent of <code>dplyr::bind_rows()</code> and
<code>dplyr::bind_cols()</code>.</p></li>

<li><p>Added <code>sdf_separate_column()</code> &ndash; this function allows one to separate
components of an array / vector column into separate scalar-valued
columns.</p></li>

<li><p><code>sdf_with_sequential_id()</code> now supports <code>from</code> parameter to choose the
starting value of the id column.</p></li>

<li><p>Added <code>sdf_pivot()</code>. This function provides a mechanism for constructing
pivot tables, using Spark&rsquo;s &lsquo;groupBy&rsquo; + &lsquo;pivot&rsquo; functionality, with a
formula interface similar to that of <code>reshape2::dcast()</code>.</p></li>
</ul>

<h3 id="mllib">MLlib</h3>

<ul>
<li><p>GLM type models now support <code>weights.column</code> to specify weights in model
fitting. (#217)</p></li>

<li><p><code>ml_logistic_regression()</code> now supports multinomial regression, in
addition to binomial regression [requires Spark 2.1.0 or greater]. (#748)</p></li>

<li><p>Implemented <code>residuals()</code> and <code>sdf_residuals()</code> for Spark linear
regression and GLM models. The former returns a R vector while
the latter returns a <code>tbl_spark</code> of training data with a <code>residuals</code>
column added.</p></li>

<li><p>Added <code>ml_model_data()</code>, used for extracting data associated with
Spark ML models.</p></li>

<li><p>The <code>ml_save()</code> and <code>ml_load()</code> functions gain a <code>meta</code> argument, allowing
users to specify where R-level model metadata should be saved independently
of the Spark model itself. This should help facilitate the saving and loading
of Spark models used in non-local connection scenarios.</p></li>

<li><p><code>ml_als_factorization()</code> now supports the implicit matrix factorization
and nonnegative least square options.</p></li>

<li><p>Added <code>ft_count_vectorizer()</code>. This function can be used to transform
columns of a Spark DataFrame so that they might be used as input to <code>ml_lda()</code>.
This should make it easier to invoke <code>ml_lda()</code> on Spark data sets.</p></li>
</ul>

<h3 id="broom">Broom</h3>

<ul>
<li>Implemented <code>tidy()</code>, <code>augment()</code>, and <code>glance()</code> from tidyverse/broom for
<code>ml_model_generalized_linear_regression</code> and <code>ml_model_linear_regression</code>
models.
<br /></li>
</ul>

<h3 id="r-compatibility">R Compatibility</h3>

<ul>
<li>Implemented <code>cbind.tbl_spark()</code>. This method works by first generating
index columns using <code>sdf_with_sequential_id()</code> then performing <code>inner_join()</code>.
Note that dplyr <code>_join()</code> functions should still be used for DataFrames
with common keys since they are less expensive.</li>
</ul>

<h3 id="connections">Connections</h3>

<ul>
<li><p>Added <code>spark_context_config()</code> and <code>hive_context_config()</code> to retrieve
runtime configurations for the Spark and Hive contexts.</p></li>

<li><p>Added <code>sparklyr.log.console</code> to redirect logs to console, useful
to troubleshooting <code>spark_connect</code>.</p></li>

<li><p>Added <code>sparklyr.backend.args</code> as config option to enable passing
parameters to the <code>sparklyr</code> backend.</p></li>

<li><p>Improved logging while establishing connections to <code>sparklyr</code>.</p></li>

<li><p>Improved <code>spark_connect()</code> performance.</p></li>

<li><p>Implemented new configuration checks to proactively report connection errors
in Windows.</p></li>

<li><p>While connecting to spark from Windows, setting the <code>sparklyr.verbose</code> option
to <code>TRUE</code> prints detailed configuration steps.</p></li>
</ul>

<h3 id="compilation">Compilation</h3>

<ul>
<li><p>Added support for <code>jar_dep</code> in the compilation specification to
support additional <code>jars</code> through <code>spark_compile()</code>.</p></li>

<li><p><code>spark_compile()</code> now prints deprecation warnings.</p></li>

<li><p>Added <code>download_scalac()</code> to assist downloading all the Scala compilers
required to build using <code>compile_package_jars</code> and provided support for
using any <code>scalac</code> minor versions while looking for the right compiler.</p></li>
</ul>

<h3 id="backend">Backend</h3>

<ul>
<li>Improved backend logging by adding type and session id prefix.</li>
</ul>

<h3 id="miscellaneous">Miscellaneous</h3>

<ul>
<li><p>Implemented <code>type_sum.jobj()</code> (from tibble) to enable better printing of jobj
objects embedded in data frames.</p></li>

<li><p>Added the <code>spark_home_set()</code> function, to help facilitate the setting of the
<code>SPARK_HOME</code> environment variable. This should prove useful in teaching
environments, when teaching the basics of Spark and sparklyr.</p></li>

<li><p>Added support for the <code>sparklyr.ui.connections</code> option, which adds additional
connection options into the new connections dialog. The
<code>rstudio.spark.connections</code> option is now deprecated.</p></li>

<li><p>Implemented the &ldquo;New Connection Dialog&rdquo; as a Shiny application to be able to
support newer versions of RStudio that deprecate current connections UI.</p></li>
</ul>

<h3 id="bug-fixes">Bug Fixes</h3>

<ul>
<li><p>Fixed <code>Path does not exist</code> referencing <code>hdfs</code> exception during <code>copy_to</code> under
systems configured with <code>HADOOP_HOME</code>.</p></li>

<li><p>Fixed session crash after &ldquo;No status is returned&rdquo; error by terminating
invalid connection and added support to print log trace during this error.</p></li>

<li><p><code>compute()</code> now caches data in memory by default. To revert this beavior use
<code>sparklyr.dplyr.compute.nocache</code> set to <code>TRUE</code>.</p></li>

<li><p><code>spark_connect()</code> with <code>master = &quot;local&quot;</code> and a given <code>version</code> overrides
<code>SPARK_HOME</code> to avoid existing installation mismatches.</p></li>

<li><p>Fixed <code>spark_connect()</code> under Windows issue when <code>newInstance0</code> is present in
the logs.</p></li>

<li><p>Fixed collecting <code>long</code> type columns when NAs are present (#463).</p></li>

<li><p>Fixed backend issue that affects systems where <code>localhost</code> does
not resolve properly to the loopback address.</p></li>

<li><p>Fixed issue collecting data frames containing newlines <code>\n</code>.</p></li>

<li><p>Spark Null objects (objects of class NullType) discovered within numeric
vectors are now collected as NAs, rather than lists of NAs.</p></li>

<li><p>Fixed warning while connecting with livy and improved 401 message.</p></li>

<li><p>Fixed issue in <code>spark_read_parquet()</code> and other read methods in which
<code>spark_normalize_path()</code> would not work in some platforms while loading
data using custom protocols like <code>s3n://</code> for Amazon S3.</p></li>

<li><p>Resolved issue in <code>spark_save()</code> / <code>load_table()</code> to support saving / loading
data and added path parameter in <code>spark_load_table()</code> for consistency with
other functions.</p></li>
</ul>

<h1 id="sparklyr-0-5-5">Sparklyr 0.5.5</h1>

<ul>
<li>Implemented support for <code>connectionViewer</code> interface required in RStudio 1.1
and <code>spark_connect</code> with <code>mode=&quot;databricks&quot;</code>.</li>
</ul>

<h1 id="sparklyr-0-5-4">Sparklyr 0.5.4</h1>

<ul>
<li>Implemented support for <code>dplyr 0.6</code> and Spark 2.1.x.
<br /></li>
</ul>

<h1 id="sparklyr-0-5-3">Sparklyr 0.5.3</h1>

<ul>
<li>Implemented support for <code>DBI 0.6</code>.</li>
</ul>

<h1 id="sparklyr-0-5-2">Sparklyr 0.5.2</h1>

<ul>
<li><p>Fix to <code>spark_connect</code> affecting Windows users and Spark 1.6.x.</p></li>

<li><p>Fix to Livy connections which would cause connections to fail while connection is on &lsquo;waiting&rsquo; state.</p></li>
</ul>

<h1 id="sparklyr-0-5-0">Sparklyr 0.5.0</h1>

<ul>
<li><p>Implemented basic authorization for Livy connections using
<code>livy_config_auth()</code>.</p></li>

<li><p>Added support to specify additional <code>spark-submit</code> parameters using the
<code>sparklyr.shell.args</code> environment variable.</p></li>

<li><p>Renamed <code>sdf_load()</code> and <code>sdf_save()</code> to <code>spark_read()</code> and <code>spark_write()</code>
for consistency.</p></li>

<li><p>The functions <code>tbl_cache()</code> and <code>tbl_uncache()</code> can now be using without
requiring the <code>dplyr</code> namespace to be loaded.</p></li>

<li><p><code>spark_read_csv(..., columns = &lt;...&gt;, header = FALSE)</code> should now work as
expected &ndash; previously, <code>sparklyr</code> would still attempt to normalize the
column names provided.</p></li>

<li><p>Support to configure Livy using the <code>livy.</code> prefix in the <code>config.yml</code> file.</p></li>

<li><p>Implemented experimental support for Livy through: <code>livy_install()</code>,
<code>livy_service_start()</code>, <code>livy_service_stop()</code> and
<code>spark_connect(method = &quot;livy&quot;)</code>.</p></li>

<li><p>The <code>ml</code> routines now accept <code>data</code> as an optional argument, to support
calls of the form e.g. <code>ml_linear_regression(y ~ x, data = data)</code>. This
should be especially helpful in conjunction with <code>dplyr::do()</code>.</p></li>

<li><p>Spark <code>DenseVector</code> and <code>SparseVector</code> objects are now deserialized as
R numeric vectors, rather than Spark objects. This should make it easier
to work with the output produced by <code>sdf_predict()</code> with Random Forest
models, for example.</p></li>

<li><p>Implemented <code>dim.tbl_spark()</code>. This should ensure that <code>dim()</code>, <code>nrow()</code>
and <code>ncol()</code> all produce the expected result with <code>tbl_spark</code>s.</p></li>

<li><p>Improved Spark 2.0 installation in Windows by creating <code>spark-defaults.conf</code>
and configuring <code>spark.sql.warehouse.dir</code>.</p></li>

<li><p>Embedded Apache Spark package dependencies to avoid requiring internet
connectivity while connecting for the first through <code>spark_connect</code>. The
<code>sparklyr.csv.embedded</code> config setting was added to configure a regular
expression to match Spark versions where the embedded package is deployed.</p></li>

<li><p>Increased exception callstack and message length to include full
error details when an exception is thrown in Spark.</p></li>

<li><p>Improved validation of supported Java versions.</p></li>

<li><p>The <code>spark_read_csv()</code> function now accepts the <code>infer_schema</code> parameter,
controlling whether the columns schema should be inferred from the underlying
file itself. Disabling this should improve performance when the schema is
known beforehand.</p></li>

<li><p>Added a <code>do_.tbl_spark</code> implementation, allowing for the execution of
<code>dplyr::do</code> statements on Spark DataFrames. Currently, the computation is
performed in serial across the different groups specified on the Spark
DataFrame; in the future we hope to explore a parallel implementation.
Note that <code>do_</code> always returns a <code>tbl_df</code> rather than a <code>tbl_spark</code>, as
the objects produced within a <code>do_</code> query may not necessarily be Spark
objects.</p></li>

<li><p>Improved errors, warnings and fallbacks for unsupported Spark versions.</p></li>

<li><p><code>sparklyr</code> now defaults to <code>tar = &quot;internal&quot;</code> in its calls to <code>untar()</code>.
This should help resolve issues some Windows users have seen related to
an inability to connect to Spark, which ultimately were caused by a lack
of permissions on the Spark installation.</p></li>

<li><p>Resolved an issue where <code>copy_to()</code> and other R =&gt; Spark data transfer
functions could fail when the last column contained missing / empty values.
(#265)</p></li>

<li><p>Added <code>sdf_persist()</code> as a wrapper to the Spark DataFrame <code>persist()</code> API.</p></li>

<li><p>Resolved an issue where <code>predict()</code> could produce results in the wrong
order for large Spark DataFrames.</p></li>

<li><p>Implemented support for <code>na.action</code> with the various Spark ML routines. The
value of <code>getOption(&quot;na.action&quot;)</code> is used by default. Users can customize the
<code>na.action</code> argument through the <code>ml.options</code> object accepted by all ML
routines.</p></li>

<li><p>On Windows, long paths, and paths containing spaces, are now supported within
calls to <code>spark_connect()</code>.</p></li>

<li><p>The <code>lag()</code> window function now accepts numeric values for <code>n</code>. Previously,
only integer values were accepted. (#249)</p></li>

<li><p>Added support to configure Ppark environment variables using <code>spark.env.*</code> config.</p></li>

<li><p>Added support for the <code>Tokenizer</code> and <code>RegexTokenizer</code> feature transformers.
These are exported as the <code>ft_tokenizer()</code> and <code>ft_regex_tokenizer()</code> functions.</p></li>

<li><p>Resolved an issue where attempting to call <code>copy_to()</code> with an R <code>data.frame</code>
containing many columns could fail with a Java StackOverflow. (#244)</p></li>

<li><p>Resolved an issue where attempting to call <code>collect()</code> on a Spark DataFrame
containing many columns could produce the wrong result. (#242)</p></li>

<li><p>Added support to parameterize network timeouts using the
<code>sparklyr.backend.timeout</code>, <code>sparklyr.gateway.start.timeout</code> and
<code>sparklyr.gateway.connect.timeout</code> config settings.</p></li>

<li><p>Improved logging while establishing connections to <code>sparklyr</code>.</p></li>

<li><p>Added <code>sparklyr.gateway.port</code> and <code>sparklyr.gateway.address</code> as config settings.</p></li>

<li><p>The <code>spark_log()</code> function now accepts the <code>filter</code> parameter. This can be used
to filter entries within the Spark log.</p></li>

<li><p>Increased network timeout for <code>sparklyr.backend.timeout</code>.</p></li>

<li><p>Moved <code>spark.jars.default</code> setting from options to Spark config.</p></li>

<li><p><code>sparklyr</code> now properly respects the Hive metastore directory with the
<code>sdf_save_table()</code> and <code>sdf_load_table()</code> APIs for Spark &lt; 2.0.0.</p></li>

<li><p>Added <code>sdf_quantile()</code> as a means of computing (approximate) quantiles
for a column of a Spark DataFrame.</p></li>

<li><p>Added support for <code>n_distinct(...)</code> within the <code>dplyr</code> interface, based on
call to Hive function <code>count(DISTINCT ...)</code>. (#220)</p></li>
</ul>

<h1 id="sparklyr-0-4-0">Sparklyr 0.4.0</h1>

<ul>
<li>First release to CRAN.</li>
</ul>

		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = '';
      var repo_id  = '';
    
    </script>

    <script src="../javascripts/application.js"></script>
    

    <script>
      
      var headers   = document.querySelectorAll('div.level2');
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            if(headers[i].id.length > 0){
              var li = document.createElement("li");
              li.setAttribute("class", "anchor");
              var a  = document.createElement("a");
              a.setAttribute("href", "#" + headers[i].id);
              a.setAttribute("title", headers[i].getElementsByTagName("h2")[0].innerHTM);
              a.innerHTML = headers[i].getElementsByTagName("h2")[0].innerHTML;
              li.appendChild(a)
              scrollspy.appendChild(li);
            }            
            
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }}
    </script>

    

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

